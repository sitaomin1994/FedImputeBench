
global_epoch: 100
model_converge_tol: 0.0001
model_converge_patience: 10
local_epoch: 5
batch_size: 32
lr: 0.001
weight_decay: 0.0001
optimizer: 'adam'
log_interval: 1