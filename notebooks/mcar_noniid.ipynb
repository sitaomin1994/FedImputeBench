{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\research\\fedmiss_bench\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from src.loaders.load_data import load_data\n",
    "from src.loaders.load_data_partition import load_data_partition, separate_data\n",
    "from src.modules.missing_simulate.mcar_simulate import simulate_nan_mcar\n",
    "from src.modules.missing_simulate.mar_simulate import simulate_nan_mar_sigmoid, simulate_nan_mar_quantile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13008, 26)\n",
      "(5854, 26) (1301, 26)\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data, config = load_data('codon', test_size=0.1)\n",
    "train_data = pd.DataFrame(train_data).sample(frac=0.5).values\n",
    "print(train_data.shape, test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client data size does not meet the minimum requirement 50. Try allocating again for the 2-th time.\n",
      "Client data size does not meet the minimum requirement 50. Try allocating again for the 3-th time.\n",
      "Client data size does not meet the minimum requirement 50. Try allocating again for the 4-th time.\n",
      "Client data size does not meet the minimum requirement 50. Try allocating again for the 5-th time.\n",
      "Client 0\t Size of data: 554\t Labels:  [3. 4. 5. 6. 9.]\n",
      "\t\t Samples of labels:  [(3, 1), (4, 316), (5, 23), (6, 213), (9, 1)]\n",
      "--------------------------------------------------\n",
      "Client 1\t Size of data: 523\t Labels:  [1. 3. 5. 6. 7.]\n",
      "\t\t Samples of labels:  [(1, 47), (3, 2), (5, 4), (6, 386), (7, 84)]\n",
      "--------------------------------------------------\n",
      "Client 2\t Size of data: 535\t Labels:  [5.]\n",
      "\t\t Samples of labels:  [(5, 535)]\n",
      "--------------------------------------------------\n",
      "Client 3\t Size of data: 330\t Labels:  [5. 6. 8. 9.]\n",
      "\t\t Samples of labels:  [(5, 4), (6, 233), (8, 92), (9, 1)]\n",
      "--------------------------------------------------\n",
      "Client 4\t Size of data: 1305\t Labels:  [0. 2.]\n",
      "\t\t Samples of labels:  [(0, 2), (2, 1303)]\n",
      "--------------------------------------------------\n",
      "Client 5\t Size of data: 224\t Labels:  [2. 7. 8. 9.]\n",
      "\t\t Samples of labels:  [(2, 43), (7, 178), (8, 2), (9, 1)]\n",
      "--------------------------------------------------\n",
      "Client 6\t Size of data: 1260\t Labels:  [0.]\n",
      "\t\t Samples of labels:  [(0, 1260)]\n",
      "--------------------------------------------------\n",
      "Client 7\t Size of data: 120\t Labels:  [0. 6. 7.]\n",
      "\t\t Samples of labels:  [(0, 3), (6, 112), (7, 5)]\n",
      "--------------------------------------------------\n",
      "Client 8\t Size of data: 752\t Labels:  [3. 4.]\n",
      "\t\t Samples of labels:  [(3, 54), (4, 698)]\n",
      "--------------------------------------------------\n",
      "Client 9\t Size of data: 251\t Labels:  [0. 1. 2. 3. 4. 5. 6. 7. 8. 9.]\n",
      "\t\t Samples of labels:  [(0, 19), (1, 1), (2, 1), (3, 32), (4, 116), (5, 18), (6, 1), (7, 1), (8, 1), (9, 61)]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "X, y, stats = separate_data(\n",
    "    train_data, num_clients = 10, niid = True, alpha = 0.05, least_samples = 50, local_test_ratio=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ms = []\n",
    "for X_origin in X:\n",
    "    X_ms.append(simulate_nan_mcar(X_origin, cols = np.arange(X_origin.shape[1]), missing_ratio=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ms = []\n",
    "seeds = 0 \n",
    "for X_origin in X:\n",
    "    X_ms.append(\n",
    "        simulate_nan_mar_sigmoid(\n",
    "            X_origin, cols = np.arange(X_origin.shape[1] - 3), missing_ratio=0.5, obs = True, strict = True, k = 'all',\n",
    "            missing_func=str(np.random.choice(['left', 'right']))\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Client:\n",
    "    def __init__(self, X, X_ms, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.X_ms = X_ms\n",
    "        self.mask = np.isnan(X_ms)\n",
    "        self.X_imp = X_ms.copy()\n",
    "        self.X_imp[self.mask] = 0\n",
    "        self.imp_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stdlib\n",
    "from typing import Any, List, Dict, Tuple\n",
    "\n",
    "# third party\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.distributions as td\n",
    "\n",
    "# hyperimpute absolute\n",
    "from emf.reproduce_utils import set_seed\n",
    "from src.imputation.models.vae_models.decoder import GaussianDecoder, BernoulliDecoder, StudentTDecoder\n",
    "from src.imputation.models.vae_models.encoder import BaseEncoder\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def weights_init(layer: Any) -> None:\n",
    "    if type(layer) == nn.Linear:\n",
    "        torch.nn.init.orthogonal_(layer.weight)\n",
    "\n",
    "\n",
    "class MIWAE(nn.Module):\n",
    "    \"\"\"MIWAE imputation plugin\n",
    "\n",
    "    Args:\n",
    "        n_epochs: int\n",
    "            Number of training iterations\n",
    "        batch_size: int\n",
    "            Batch size\n",
    "        latent_size: int\n",
    "            dimension of the latent space\n",
    "        n_hidden: int\n",
    "            number of hidden units\n",
    "        K: int\n",
    "            number of IS during training\n",
    "        random_state: int\n",
    "            random seed\n",
    "\n",
    "    Reference: \"MIWAE: Deep Generative Modelling and Imputation of Incomplete Data\", Pierre-Alexandre Mattei,\n",
    "    Jes Frellsen\n",
    "    Original code: https://github.com/pamattei/miwae\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_features: int,\n",
    "            latent_size: int = 1,\n",
    "            n_hidden: int = 16,\n",
    "            n_hidden_layers: int = 2,\n",
    "            seed: int = 0,\n",
    "            out_dist='studentt',\n",
    "            K: int = 20,\n",
    "            L: int = 1000,\n",
    "    ) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "        set_seed(seed)\n",
    "\n",
    "        # parameters\n",
    "        self.num_features = num_features\n",
    "        self.n_hidden = n_hidden  # number of hidden units in (same for all MLPs)\n",
    "        self.n_hidden_layers = n_hidden_layers  # number of hidden layers in (same for all MLPs)\n",
    "        self.latent_size = latent_size  # dimension of the latent space\n",
    "        self.K = K  # number of IS during training\n",
    "        self.L = L  # number of samples for imputation\n",
    "\n",
    "        # encoder\n",
    "        self.encoder = BaseEncoder(\n",
    "            self.num_features, self.latent_size, [self.n_hidden for _ in range(self.n_hidden_layers)],\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        # decoder\n",
    "        self.out_dist = out_dist\n",
    "        if out_dist == 'studentt':\n",
    "            self.decoder = StudentTDecoder(\n",
    "                self.latent_size, self.num_features, [self.n_hidden for _ in range(self.n_hidden_layers)],\n",
    "            )\n",
    "        elif out_dist == 'gaussian':\n",
    "            self.decoder = GaussianDecoder(\n",
    "                self.latent_size, self.num_features, [self.n_hidden for _ in range(self.n_hidden_layers)],\n",
    "            )\n",
    "        elif out_dist == 'bernoulli':\n",
    "            self.decoder = BernoulliDecoder(\n",
    "                self.latent_size, self.num_features, [self.n_hidden for _ in range(self.n_hidden_layers)],\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"Invalid output distribution\")\n",
    "\n",
    "        self.decoder = self.decoder.to(DEVICE)\n",
    "        \n",
    "        # mapping z to mask using hint\n",
    "        self.mask_net = nn.Sequential(\n",
    "            nn.Linear(self.latent_size, self.n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.n_hidden, self.n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.n_hidden, self.num_features),\n",
    "            nn.Sigmoid()\n",
    "        ).to(DEVICE)\n",
    "        \n",
    "        self.z_L = None\n",
    "        self.mask_L = None\n",
    "\n",
    "        # prior for z\n",
    "        self.p_z = td.Independent(\n",
    "            td.Normal(loc=torch.zeros(self.latent_size).to(DEVICE), scale=torch.ones(self.latent_size).to(DEVICE)), 1\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def name() -> str:\n",
    "        return \"miwae\"\n",
    "\n",
    "    def init(self, seed):\n",
    "        set_seed(seed)\n",
    "        self.encoder.apply(weights_init)\n",
    "        self.decoder.apply(weights_init)\n",
    "        self.mask_net.apply(weights_init)\n",
    "\n",
    "    def compute_loss(self, inputs: List[torch.Tensor]) -> Tuple[torch.Tensor, Dict]:\n",
    "        x, mask = inputs  # x - data, mask - missing mask\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        # encoder\n",
    "        mu, logvar = self.encoder(x)\n",
    "\n",
    "        q_zgivenxobs = td.Independent(td.Normal(loc=mu, scale=logvar), 1)\n",
    "        zgivenx = q_zgivenxobs.rsample([self.K])  # shape (K, batch_size, latent_size)\n",
    "        zgivenx_flat = zgivenx.reshape([self.K * batch_size, self.latent_size])\n",
    "        \n",
    "        self.z_L = zgivenx_flat\n",
    "\n",
    "        # decoder\n",
    "        out_decoder = self.decoder(zgivenx_flat)\n",
    "        recon_x_means = self.decoder.l_out_mu(out_decoder)\n",
    "\n",
    "        # compute loss\n",
    "        data_flat = torch.Tensor.repeat(x, [self.K, 1]).reshape([-1, 1]).to(DEVICE)\n",
    "        tiled_mask = torch.Tensor.repeat(mask, [self.K, 1]).to(DEVICE)\n",
    "        self.mask_L = tiled_mask\n",
    "\n",
    "        # p(x|z)\n",
    "        all_log_pxgivenz_flat = self.decoder.dist_xgivenz(out_decoder, flat=True).log_prob(data_flat)\n",
    "        all_log_pxgivenz = all_log_pxgivenz_flat.reshape([self.K * batch_size, self.num_features])\n",
    "        logpxobsgivenz = torch.sum(all_log_pxgivenz * tiled_mask, 1).reshape([self.K, batch_size])\n",
    "\n",
    "        # p(z) and q(z|x)\n",
    "        logpz = self.p_z.log_prob(zgivenx)\n",
    "        logq = q_zgivenxobs.log_prob(zgivenx)\n",
    "\n",
    "        neg_bound = -torch.mean(torch.logsumexp(logpxobsgivenz + logpz - logq, 0))\n",
    "\n",
    "        return neg_bound, {}\n",
    "    \n",
    "    def mask_prediction_loss(self, x, mask):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            batch_size = x.shape[0]\n",
    "            mu, logvar = self.encoder(x)\n",
    "            q_zgivenxobs = td.Independent(td.Normal(loc=mu, scale=logvar), 1)\n",
    "            zgivenx = q_zgivenxobs.rsample([self.K])  # shape (K, batch_size, latent_size)\n",
    "            zgivenx_flat = zgivenx.reshape([self.K * batch_size, self.latent_size])\n",
    "            z = zgivenx_flat\n",
    "            tiled_mask = torch.Tensor.repeat(mask, [self.K, 1]).to(DEVICE)\n",
    "            mask = tiled_mask.reshape([-1, self.num_features]).int()\n",
    "            mask = mask.float()\n",
    "            #z = torch.cat([z, mask], 1)\n",
    "        \n",
    "        mask_pred = self.mask_net(z)\n",
    "        loss = torch.nn.BCELoss()(mask_pred, mask)  # predict missing prob -> 1\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def mask_prediction(self, x: torch.Tensor, mask: torch.Tensor, K: int = 10):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            mu, logvar = self.encoder(x)\n",
    "            q_zgivenxobs = td.Independent(td.Normal(loc=mu, scale=logvar), 1)\n",
    "            zgivenx = q_zgivenxobs.rsample([K])  # shape (K, batch_size, latent_size)\n",
    "            zgivenx_flat = zgivenx.reshape([K * x.shape[0], self.latent_size])\n",
    "            mask_L = torch.Tensor.repeat(mask, [K, 1])\n",
    "            #zgivenx_flat = torch.cat([zgivenx_flat, mask_L.reshape([-1, self.num_features]).float()], 1)\n",
    "            \n",
    "            mask_pred = self.mask_net(zgivenx_flat)\n",
    "            mask_pred = mask_pred.reshape([K, x.shape[0], self.num_features])\n",
    "            \n",
    "            return mask_pred\n",
    "\n",
    "    def impute(self, x: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        L = self.L\n",
    "        batch_size = x.shape[0]\n",
    "        p = x.shape[1]\n",
    "\n",
    "        # encoder\n",
    "        self.encoder.to(DEVICE)\n",
    "        self.decoder.to(DEVICE)\n",
    "        mu, logvar = self.encoder(x)\n",
    "        q_zgivenxobs = td.Independent(td.Normal(loc=mu, scale=logvar), 1)\n",
    "\n",
    "        zgivenx = q_zgivenxobs.rsample([L])\n",
    "        zgivenx_flat = zgivenx.reshape([L * batch_size, self.latent_size])\n",
    "\n",
    "        # decoder\n",
    "        out_decoder = self.decoder(zgivenx_flat)\n",
    "        recon_x_means = self.decoder.l_out_mu(out_decoder)\n",
    "\n",
    "        # loss\n",
    "        data_flat = torch.Tensor.repeat(x, [L, 1]).reshape([-1, 1]).to(DEVICE)\n",
    "        tiledmask = torch.Tensor.repeat(mask, [L, 1]).to(DEVICE)\n",
    "\n",
    "        all_log_pxgivenz_flat = self.decoder.dist_xgivenz(out_decoder, flat=True).log_prob(data_flat)\n",
    "        all_log_pxgivenz = all_log_pxgivenz_flat.reshape([L * batch_size, p])\n",
    "\n",
    "        logpxobsgivenz = torch.sum(all_log_pxgivenz * tiledmask, 1).reshape([L, batch_size])\n",
    "        logpz = self.p_z.log_prob(zgivenx)\n",
    "        logq = q_zgivenxobs.log_prob(zgivenx)\n",
    "\n",
    "        # imputation weighted samples\n",
    "        imp_weights = torch.nn.functional.softmax(\n",
    "            logpxobsgivenz + logpz - logq, 0\n",
    "        )  # these are w_1,....,w_L for all observations in the batch\n",
    "\n",
    "        xgivenz = self.decoder.imp_dist_xgivenz(out_decoder)\n",
    "        xms = xgivenz.sample().reshape([L, batch_size, p])\n",
    "        xm = torch.einsum(\"ki,kij->ij\", imp_weights, xms)\n",
    "\n",
    "        # merge imputed values with observed values\n",
    "        xhat = torch.clone(x)\n",
    "        xhat[~mask.bool()] = xm[~mask.bool()]\n",
    "\n",
    "        return xhat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from copy import deepcopy\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "from src.evaluation.imp_quality_metrics import rmse\n",
    "\n",
    "def local_train(model, X, mask, train_params, X_true, return_params = False):\n",
    "    \n",
    "    model.to(DEVICE)\n",
    "    lr = train_params['lr']\n",
    "    weight_decay = train_params['weight_decay']\n",
    "    epochs = train_params['epochs']\n",
    "    batch_size = train_params['batch_size']\n",
    "    verbose = train_params['verbose']\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay, amsgrad=True)\n",
    "\n",
    "    # data\n",
    "    n = X.shape[0]\n",
    "    X_imp = X.copy()\n",
    "    X_mask = mask.copy()\n",
    "    bs = min(batch_size, n)\n",
    "\n",
    "    final_loss = 0\n",
    "    rmses = []\n",
    "    for ep in range(epochs):\n",
    "        \n",
    "        # evaluation\n",
    "        with torch.no_grad():\n",
    "            X_imp_new = model.impute(\n",
    "                torch.from_numpy(X_imp).float().to(DEVICE), torch.from_numpy(~X_mask).float().to(DEVICE)\n",
    "            )\n",
    "            rmse_value = rmse(X_imp_new.detach().clone().cpu().numpy(), X_true, X_mask)\n",
    "            rmses.append(rmse_value)\n",
    "\n",
    "        # shuffle data\n",
    "        perm = np.random.permutation(n)  # We use the \"random reshuffling\" version of SGD\n",
    "        batches_data = np.array_split(X_imp[perm,], int(n / bs), )\n",
    "        batches_mask = np.array_split(X_mask[perm,], int(n / bs), )\n",
    "        total_loss, total_iters = 0, 0\n",
    "        total_mask_loss = 0\n",
    "        model.train()\n",
    "        for it in range(len(batches_data)):\n",
    "            optimizer.zero_grad()\n",
    "            model.encoder.zero_grad()\n",
    "            model.decoder.zero_grad()\n",
    "            model.mask_net.zero_grad()\n",
    "            b_data = torch.from_numpy(batches_data[it]).float().to(DEVICE)\n",
    "            b_mask = torch.from_numpy(~batches_mask[it]).float().to(DEVICE)\n",
    "            data = [b_data, b_mask]\n",
    "            loss, ret_dict = model.compute_loss(data)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_iters += 1\n",
    "\n",
    "\n",
    "        # print loss\n",
    "        if (ep + 1) % verbose == 0:\n",
    "            print('Epoch %s/%s, Loss = %s RMSE = %s' % (ep, epochs, total_loss / total_iters, rmses[-1]))\n",
    "\n",
    "        if DEVICE == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        final_loss = total_loss / total_iters\n",
    "\n",
    "    model.to(\"cpu\")\n",
    "    \n",
    "    if return_params:\n",
    "        return deepcopy(model.state_dict())\n",
    "    else:\n",
    "        return model\n",
    "\n",
    "def impute(model, X, mask):\n",
    "    model.to(DEVICE)\n",
    "    X_imp = X.copy()\n",
    "    X_mask = mask.copy()\n",
    "    X_imp[mask] = 0\n",
    "    X_imp = torch.from_numpy(X_imp).float().to(DEVICE)\n",
    "    X_mask = torch.from_numpy(~X_mask).float().to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        X_imp_new = model.impute(X_imp, X_mask)\n",
    "    model.to(\"cpu\")\n",
    "    return X_imp_new.detach().clone().cpu().numpy()\n",
    "\n",
    "\n",
    "def evaluate_model(clients):\n",
    "    rmses = []\n",
    "    for client in clients:\n",
    "        client.X_imp = impute(client.imp_model, client.X_ms, client.mask)\n",
    "        rmses.append(rmse(client.X_imp, client.X, client.mask))\n",
    "    print(\"RMSE Mean: {:.4f} Std: {:.4f} Max: {:.4f} Min: {:.4f} \".format(\n",
    "        np.mean(rmses), np.std(rmses), np.max(rmses), np.min(rmses)\n",
    "    ))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clients = [Client(X[i].copy(), X_ms[i].copy(), y[i].copy()) for i in range(len(X))]\n",
    "for idx, client in enumerate(clients):\n",
    "    client.imp_model = MIWAE(\n",
    "        client.X.shape[1], latent_size=10, n_hidden=32, n_hidden_layers=2, seed=idx, out_dist='gaussian', K=20, L=100, \n",
    "    )\n",
    "    \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.heatmap(clients[0].mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global iteration 0\n",
      "RMSE Mean: 0.2131 Std: 0.0616 Max: 0.3263 Min: 0.1110 \n",
      "Global iteration 1\n",
      "RMSE Mean: 0.1827 Std: 0.0476 Max: 0.2979 Min: 0.1057 \n",
      "Global iteration 2\n",
      "RMSE Mean: 0.1719 Std: 0.0407 Max: 0.2765 Min: 0.1086 \n",
      "Global iteration 3\n",
      "RMSE Mean: 0.1653 Std: 0.0360 Max: 0.2555 Min: 0.1048 \n",
      "Global iteration 4\n",
      "RMSE Mean: 0.1607 Std: 0.0290 Max: 0.2282 Min: 0.1067 \n",
      "Global iteration 5\n",
      "RMSE Mean: 0.1585 Std: 0.0248 Max: 0.2058 Min: 0.1046 \n",
      "Global iteration 6\n",
      "RMSE Mean: 0.1564 Std: 0.0225 Max: 0.1906 Min: 0.1020 \n",
      "Global iteration 7\n",
      "RMSE Mean: 0.1537 Std: 0.0209 Max: 0.1748 Min: 0.1018 \n",
      "Global iteration 8\n",
      "RMSE Mean: 0.1524 Std: 0.0195 Max: 0.1709 Min: 0.1027 \n",
      "Global iteration 9\n",
      "RMSE Mean: 0.1509 Std: 0.0189 Max: 0.1702 Min: 0.1027 \n",
      "Global iteration 10\n",
      "RMSE Mean: 0.1512 Std: 0.0196 Max: 0.1733 Min: 0.1021 \n",
      "Global iteration 11\n",
      "RMSE Mean: 0.1501 Std: 0.0191 Max: 0.1743 Min: 0.1025 \n",
      "Global iteration 12\n",
      "RMSE Mean: 0.1506 Std: 0.0184 Max: 0.1732 Min: 0.1038 \n",
      "Global iteration 13\n",
      "RMSE Mean: 0.1506 Std: 0.0189 Max: 0.1710 Min: 0.1018 \n",
      "Global iteration 14\n",
      "RMSE Mean: 0.1493 Std: 0.0186 Max: 0.1704 Min: 0.1017 \n",
      "Global iteration 15\n",
      "RMSE Mean: 0.1499 Std: 0.0187 Max: 0.1695 Min: 0.1014 \n",
      "Global iteration 16\n",
      "RMSE Mean: 0.1493 Std: 0.0183 Max: 0.1690 Min: 0.1017 \n",
      "Global iteration 17\n",
      "RMSE Mean: 0.1485 Std: 0.0188 Max: 0.1713 Min: 0.1013 \n",
      "Global iteration 18\n",
      "RMSE Mean: 0.1480 Std: 0.0180 Max: 0.1685 Min: 0.1014 \n",
      "Global iteration 19\n",
      "RMSE Mean: 0.1484 Std: 0.0184 Max: 0.1706 Min: 0.1012 \n",
      "Global iteration 20\n",
      "RMSE Mean: 0.1472 Std: 0.0173 Max: 0.1670 Min: 0.1020 \n",
      "Global iteration 21\n",
      "RMSE Mean: 0.1481 Std: 0.0180 Max: 0.1676 Min: 0.1021 \n",
      "Global iteration 22\n",
      "RMSE Mean: 0.1481 Std: 0.0178 Max: 0.1671 Min: 0.1030 \n",
      "Global iteration 23\n",
      "RMSE Mean: 0.1471 Std: 0.0180 Max: 0.1651 Min: 0.1006 \n",
      "Global iteration 24\n",
      "RMSE Mean: 0.1473 Std: 0.0177 Max: 0.1655 Min: 0.1010 \n",
      "Global iteration 25\n",
      "RMSE Mean: 0.1471 Std: 0.0169 Max: 0.1660 Min: 0.1020 \n",
      "Global iteration 26\n",
      "RMSE Mean: 0.1478 Std: 0.0174 Max: 0.1655 Min: 0.1008 \n",
      "Global iteration 27\n",
      "RMSE Mean: 0.1475 Std: 0.0178 Max: 0.1673 Min: 0.1011 \n",
      "Global iteration 28\n",
      "RMSE Mean: 0.1479 Std: 0.0180 Max: 0.1694 Min: 0.1006 \n",
      "Global iteration 29\n",
      "RMSE Mean: 0.1472 Std: 0.0175 Max: 0.1684 Min: 0.1013 \n",
      "Global iteration 30\n",
      "RMSE Mean: 0.1476 Std: 0.0176 Max: 0.1686 Min: 0.1019 \n",
      "Global iteration 31\n",
      "RMSE Mean: 0.1486 Std: 0.0178 Max: 0.1702 Min: 0.1014 \n",
      "Global iteration 32\n",
      "RMSE Mean: 0.1479 Std: 0.0177 Max: 0.1710 Min: 0.1015 \n",
      "Global iteration 33\n",
      "RMSE Mean: 0.1479 Std: 0.0179 Max: 0.1690 Min: 0.1009 \n",
      "Global iteration 34\n",
      "RMSE Mean: 0.1464 Std: 0.0178 Max: 0.1692 Min: 0.0999 \n",
      "Global iteration 35\n",
      "RMSE Mean: 0.1482 Std: 0.0184 Max: 0.1678 Min: 0.1004 \n",
      "Global iteration 36\n",
      "RMSE Mean: 0.1483 Std: 0.0183 Max: 0.1733 Min: 0.1014 \n",
      "Global iteration 37\n",
      "RMSE Mean: 0.1478 Std: 0.0184 Max: 0.1725 Min: 0.1006 \n",
      "Global iteration 38\n",
      "RMSE Mean: 0.1483 Std: 0.0184 Max: 0.1726 Min: 0.1005 \n",
      "Global iteration 39\n",
      "RMSE Mean: 0.1477 Std: 0.0182 Max: 0.1713 Min: 0.1006 \n",
      "Global iteration 40\n",
      "RMSE Mean: 0.1483 Std: 0.0182 Max: 0.1689 Min: 0.1006 \n",
      "Global iteration 41\n",
      "RMSE Mean: 0.1467 Std: 0.0181 Max: 0.1690 Min: 0.1011 \n",
      "Global iteration 42\n",
      "RMSE Mean: 0.1472 Std: 0.0174 Max: 0.1674 Min: 0.1018 \n",
      "Global iteration 43\n",
      "RMSE Mean: 0.1479 Std: 0.0187 Max: 0.1731 Min: 0.1008 \n",
      "Global iteration 44\n",
      "RMSE Mean: 0.1477 Std: 0.0176 Max: 0.1701 Min: 0.1012 \n",
      "Global iteration 45\n",
      "RMSE Mean: 0.1488 Std: 0.0194 Max: 0.1740 Min: 0.0995 \n",
      "Global iteration 46\n",
      "RMSE Mean: 0.1468 Std: 0.0183 Max: 0.1692 Min: 0.1004 \n",
      "Global iteration 47\n",
      "RMSE Mean: 0.1479 Std: 0.0181 Max: 0.1693 Min: 0.1016 \n",
      "Global iteration 48\n",
      "RMSE Mean: 0.1471 Std: 0.0183 Max: 0.1707 Min: 0.1006 \n",
      "Global iteration 49\n",
      "RMSE Mean: 0.1462 Std: 0.0180 Max: 0.1712 Min: 0.1005 \n",
      "Global iteration 50\n",
      "RMSE Mean: 0.1467 Std: 0.0174 Max: 0.1681 Min: 0.1017 \n",
      "Global iteration 51\n",
      "RMSE Mean: 0.1475 Std: 0.0180 Max: 0.1691 Min: 0.1004 \n",
      "Global iteration 52\n",
      "RMSE Mean: 0.1463 Std: 0.0176 Max: 0.1668 Min: 0.1013 \n",
      "Global iteration 53\n",
      "RMSE Mean: 0.1460 Std: 0.0174 Max: 0.1688 Min: 0.1008 \n",
      "Global iteration 54\n",
      "RMSE Mean: 0.1469 Std: 0.0174 Max: 0.1676 Min: 0.1012 \n",
      "Global iteration 55\n",
      "RMSE Mean: 0.1477 Std: 0.0173 Max: 0.1693 Min: 0.1027 \n",
      "Global iteration 56\n",
      "RMSE Mean: 0.1469 Std: 0.0181 Max: 0.1675 Min: 0.0997 \n",
      "Global iteration 57\n",
      "RMSE Mean: 0.1470 Std: 0.0179 Max: 0.1677 Min: 0.1000 \n",
      "Global iteration 58\n",
      "RMSE Mean: 0.1474 Std: 0.0176 Max: 0.1689 Min: 0.1006 \n",
      "Global iteration 59\n",
      "RMSE Mean: 0.1460 Std: 0.0174 Max: 0.1675 Min: 0.1013 \n",
      "Global iteration 60\n",
      "RMSE Mean: 0.1461 Std: 0.0171 Max: 0.1663 Min: 0.1008 \n",
      "Global iteration 61\n",
      "RMSE Mean: 0.1462 Std: 0.0176 Max: 0.1662 Min: 0.1004 \n",
      "Global iteration 62\n",
      "RMSE Mean: 0.1459 Std: 0.0180 Max: 0.1675 Min: 0.0998 \n",
      "Global iteration 63\n",
      "RMSE Mean: 0.1453 Std: 0.0173 Max: 0.1660 Min: 0.1002 \n",
      "Global iteration 64\n",
      "RMSE Mean: 0.1463 Std: 0.0173 Max: 0.1680 Min: 0.1009 \n",
      "Global iteration 65\n",
      "RMSE Mean: 0.1459 Std: 0.0172 Max: 0.1680 Min: 0.1013 \n",
      "Global iteration 66\n",
      "RMSE Mean: 0.1460 Std: 0.0175 Max: 0.1671 Min: 0.1006 \n",
      "Global iteration 67\n",
      "RMSE Mean: 0.1459 Std: 0.0180 Max: 0.1679 Min: 0.0999 \n",
      "Global iteration 68\n",
      "RMSE Mean: 0.1460 Std: 0.0173 Max: 0.1672 Min: 0.1010 \n",
      "Global iteration 69\n",
      "RMSE Mean: 0.1459 Std: 0.0176 Max: 0.1676 Min: 0.1010 \n",
      "Global iteration 70\n",
      "RMSE Mean: 0.1458 Std: 0.0170 Max: 0.1677 Min: 0.1016 \n",
      "Global iteration 71\n",
      "RMSE Mean: 0.1454 Std: 0.0180 Max: 0.1679 Min: 0.0989 \n",
      "Global iteration 72\n",
      "RMSE Mean: 0.1456 Std: 0.0172 Max: 0.1661 Min: 0.1006 \n",
      "Global iteration 73\n",
      "RMSE Mean: 0.1449 Std: 0.0172 Max: 0.1662 Min: 0.1008 \n",
      "Global iteration 74\n",
      "RMSE Mean: 0.1453 Std: 0.0173 Max: 0.1669 Min: 0.1015 \n",
      "Global iteration 75\n",
      "RMSE Mean: 0.1453 Std: 0.0176 Max: 0.1651 Min: 0.1006 \n",
      "Global iteration 76\n",
      "RMSE Mean: 0.1439 Std: 0.0172 Max: 0.1662 Min: 0.1011 \n",
      "Global iteration 77\n",
      "RMSE Mean: 0.1454 Std: 0.0181 Max: 0.1690 Min: 0.1006 \n",
      "Global iteration 78\n",
      "RMSE Mean: 0.1460 Std: 0.0177 Max: 0.1669 Min: 0.1008 \n",
      "Global iteration 79\n",
      "RMSE Mean: 0.1438 Std: 0.0166 Max: 0.1649 Min: 0.1005 \n",
      "Global iteration 80\n",
      "RMSE Mean: 0.1445 Std: 0.0172 Max: 0.1654 Min: 0.1005 \n",
      "Global iteration 81\n",
      "RMSE Mean: 0.1447 Std: 0.0179 Max: 0.1661 Min: 0.0998 \n",
      "Global iteration 82\n",
      "RMSE Mean: 0.1447 Std: 0.0173 Max: 0.1669 Min: 0.1006 \n",
      "Global iteration 83\n",
      "RMSE Mean: 0.1451 Std: 0.0177 Max: 0.1691 Min: 0.1002 \n",
      "Global iteration 84\n",
      "RMSE Mean: 0.1448 Std: 0.0171 Max: 0.1653 Min: 0.1012 \n",
      "Global iteration 85\n",
      "RMSE Mean: 0.1450 Std: 0.0176 Max: 0.1674 Min: 0.1009 \n",
      "Global iteration 86\n",
      "RMSE Mean: 0.1448 Std: 0.0180 Max: 0.1677 Min: 0.1004 \n",
      "Global iteration 87\n",
      "RMSE Mean: 0.1453 Std: 0.0183 Max: 0.1673 Min: 0.1005 \n",
      "Global iteration 88\n",
      "RMSE Mean: 0.1444 Std: 0.0178 Max: 0.1660 Min: 0.1013 \n",
      "Global iteration 89\n",
      "RMSE Mean: 0.1454 Std: 0.0178 Max: 0.1699 Min: 0.1009 \n",
      "Global iteration 90\n",
      "RMSE Mean: 0.1448 Std: 0.0177 Max: 0.1697 Min: 0.1009 \n",
      "Global iteration 91\n",
      "RMSE Mean: 0.1448 Std: 0.0172 Max: 0.1662 Min: 0.1020 \n",
      "Global iteration 92\n",
      "RMSE Mean: 0.1458 Std: 0.0176 Max: 0.1648 Min: 0.1013 \n",
      "Global iteration 93\n",
      "RMSE Mean: 0.1448 Std: 0.0178 Max: 0.1681 Min: 0.1012 \n",
      "Global iteration 94\n",
      "RMSE Mean: 0.1451 Std: 0.0177 Max: 0.1665 Min: 0.1013 \n",
      "Global iteration 95\n",
      "RMSE Mean: 0.1454 Std: 0.0175 Max: 0.1672 Min: 0.1016 \n",
      "Global iteration 96\n",
      "RMSE Mean: 0.1457 Std: 0.0177 Max: 0.1683 Min: 0.1015 \n",
      "Global iteration 97\n",
      "RMSE Mean: 0.1449 Std: 0.0178 Max: 0.1678 Min: 0.1012 \n",
      "Global iteration 98\n",
      "RMSE Mean: 0.1445 Std: 0.0177 Max: 0.1668 Min: 0.1006 \n",
      "Global iteration 99\n",
      "RMSE Mean: 0.1458 Std: 0.0182 Max: 0.1672 Min: 0.1017 \n"
     ]
    }
   ],
   "source": [
    "global_iteration = 100\n",
    "\n",
    "train_params = {\n",
    "    'lr': 1e-3,\n",
    "    'weight_decay': 1e-5,\n",
    "    'epochs': 5,\n",
    "    'batch_size': 64,\n",
    "    'verbose': 10\n",
    "}\n",
    "\n",
    "for it in range(global_iteration):\n",
    "    print(f\"Global iteration {it}\")\n",
    "    \n",
    "    # local model training\n",
    "    params = []\n",
    "    for client in clients:\n",
    "        params.append(\n",
    "            local_train(client.imp_model, client.X_imp, client.mask, train_params, client.X, return_params=True)\n",
    "        )\n",
    "    \n",
    "    fit_res = [{'sample_size': client.X.shape[0]} for client in clients]\n",
    "    \n",
    "    # # federated averaging\n",
    "    # avg_params, _ = fedavg(params, fit_res)\n",
    "    \n",
    "    # for idx, client in enumerate(clients):\n",
    "    #     client.imp_model.load_state_dict(avg_params[idx])\n",
    "        \n",
    "    # evaluation\n",
    "    evaluate_model(clients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FedAvg Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fedavg(local_model_parameters, fit_res):\n",
    "    # federated averaging implementation\n",
    "    averaged_model_state_dict = OrderedDict()  # global parameters\n",
    "    sample_sizes = np.array([item['sample_size'] for item in fit_res])\n",
    "    normed_weights = sample_sizes / np.sum(sample_sizes)\n",
    "\n",
    "    for it, local_model_state_dict in enumerate(local_model_parameters):\n",
    "        for key in local_model_state_dict.keys():\n",
    "            if it == 0:\n",
    "                averaged_model_state_dict[key] = normed_weights[it]*local_model_state_dict[key]\n",
    "            else:\n",
    "                averaged_model_state_dict[key] += normed_weights[it]*local_model_state_dict[key]\n",
    "\n",
    "    # copy parameters for each client\n",
    "    agg_model_parameters = [deepcopy(averaged_model_state_dict) for _ in range(len(local_model_parameters))]\n",
    "    agg_res = {}\n",
    "\n",
    "    return agg_model_parameters, agg_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "clients = [Client(X[i].copy(), X_ms[i].copy(), y[i].copy()) for i in range(len(X))]\n",
    "for idx, client in enumerate(clients):\n",
    "    client.imp_model = MIWAE(\n",
    "        client.X.shape[1], latent_size=10, n_hidden=32, n_hidden_layers=2, seed=idx, out_dist='gaussian', K=20, L=100\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global iteration 0\n",
      "RMSE Mean: 0.2328 Std: 0.0111 Max: 0.2500 Min: 0.2172 \n",
      "Global iteration 1\n",
      "RMSE Mean: 0.1556 Std: 0.0151 Max: 0.1827 Min: 0.1364 \n",
      "Global iteration 2\n",
      "RMSE Mean: 0.1553 Std: 0.0147 Max: 0.1818 Min: 0.1368 \n",
      "Global iteration 3\n",
      "RMSE Mean: 0.1552 Std: 0.0146 Max: 0.1822 Min: 0.1363 \n",
      "Global iteration 4\n",
      "RMSE Mean: 0.1548 Std: 0.0148 Max: 0.1816 Min: 0.1360 \n",
      "Global iteration 5\n",
      "RMSE Mean: 0.1549 Std: 0.0148 Max: 0.1816 Min: 0.1362 \n",
      "Global iteration 6\n",
      "RMSE Mean: 0.1552 Std: 0.0144 Max: 0.1814 Min: 0.1368 \n",
      "Global iteration 7\n",
      "RMSE Mean: 0.1555 Std: 0.0141 Max: 0.1814 Min: 0.1367 \n",
      "Global iteration 8\n",
      "RMSE Mean: 0.1548 Std: 0.0146 Max: 0.1812 Min: 0.1363 \n",
      "Global iteration 9\n",
      "RMSE Mean: 0.1552 Std: 0.0143 Max: 0.1793 Min: 0.1367 \n",
      "Global iteration 10\n",
      "RMSE Mean: 0.1552 Std: 0.0144 Max: 0.1799 Min: 0.1364 \n",
      "Global iteration 11\n",
      "RMSE Mean: 0.1558 Std: 0.0145 Max: 0.1793 Min: 0.1374 \n",
      "Global iteration 12\n",
      "RMSE Mean: 0.1558 Std: 0.0146 Max: 0.1803 Min: 0.1369 \n",
      "Global iteration 13\n",
      "RMSE Mean: 0.1566 Std: 0.0146 Max: 0.1815 Min: 0.1368 \n",
      "Global iteration 14\n",
      "RMSE Mean: 0.1562 Std: 0.0150 Max: 0.1817 Min: 0.1369 \n",
      "Global iteration 15\n",
      "RMSE Mean: 0.1568 Std: 0.0149 Max: 0.1820 Min: 0.1369 \n",
      "Global iteration 16\n",
      "RMSE Mean: 0.1563 Std: 0.0151 Max: 0.1816 Min: 0.1367 \n",
      "Global iteration 17\n",
      "RMSE Mean: 0.1563 Std: 0.0156 Max: 0.1832 Min: 0.1354 \n",
      "Global iteration 18\n",
      "RMSE Mean: 0.1570 Std: 0.0158 Max: 0.1842 Min: 0.1360 \n",
      "Global iteration 19\n",
      "RMSE Mean: 0.1565 Std: 0.0163 Max: 0.1838 Min: 0.1347 \n",
      "Global iteration 20\n",
      "RMSE Mean: 0.1560 Std: 0.0161 Max: 0.1838 Min: 0.1348 \n",
      "Global iteration 21\n",
      "RMSE Mean: 0.1562 Std: 0.0160 Max: 0.1835 Min: 0.1348 \n",
      "Global iteration 22\n",
      "RMSE Mean: 0.1562 Std: 0.0167 Max: 0.1829 Min: 0.1341 \n",
      "Global iteration 23\n",
      "RMSE Mean: 0.1565 Std: 0.0171 Max: 0.1836 Min: 0.1329 \n",
      "Global iteration 24\n",
      "RMSE Mean: 0.1562 Std: 0.0166 Max: 0.1830 Min: 0.1334 \n",
      "Global iteration 25\n",
      "RMSE Mean: 0.1561 Std: 0.0168 Max: 0.1828 Min: 0.1327 \n",
      "Global iteration 26\n",
      "RMSE Mean: 0.1562 Std: 0.0164 Max: 0.1819 Min: 0.1330 \n",
      "Global iteration 27\n",
      "RMSE Mean: 0.1559 Std: 0.0169 Max: 0.1812 Min: 0.1322 \n",
      "Global iteration 28\n",
      "RMSE Mean: 0.1562 Std: 0.0167 Max: 0.1819 Min: 0.1324 \n",
      "Global iteration 29\n",
      "RMSE Mean: 0.1562 Std: 0.0170 Max: 0.1824 Min: 0.1322 \n",
      "Global iteration 30\n",
      "RMSE Mean: 0.1556 Std: 0.0167 Max: 0.1813 Min: 0.1317 \n",
      "Global iteration 31\n",
      "RMSE Mean: 0.1555 Std: 0.0165 Max: 0.1810 Min: 0.1326 \n",
      "Global iteration 32\n",
      "RMSE Mean: 0.1556 Std: 0.0161 Max: 0.1795 Min: 0.1333 \n",
      "Global iteration 33\n",
      "RMSE Mean: 0.1549 Std: 0.0161 Max: 0.1796 Min: 0.1330 \n",
      "Global iteration 34\n",
      "RMSE Mean: 0.1553 Std: 0.0158 Max: 0.1806 Min: 0.1328 \n",
      "Global iteration 35\n",
      "RMSE Mean: 0.1556 Std: 0.0162 Max: 0.1803 Min: 0.1322 \n",
      "Global iteration 36\n",
      "RMSE Mean: 0.1555 Std: 0.0158 Max: 0.1804 Min: 0.1336 \n",
      "Global iteration 37\n",
      "RMSE Mean: 0.1561 Std: 0.0159 Max: 0.1801 Min: 0.1340 \n",
      "Global iteration 38\n",
      "RMSE Mean: 0.1558 Std: 0.0154 Max: 0.1792 Min: 0.1336 \n",
      "Global iteration 39\n",
      "RMSE Mean: 0.1557 Std: 0.0154 Max: 0.1794 Min: 0.1342 \n",
      "Global iteration 40\n",
      "RMSE Mean: 0.1555 Std: 0.0159 Max: 0.1792 Min: 0.1344 \n",
      "Global iteration 41\n",
      "RMSE Mean: 0.1556 Std: 0.0157 Max: 0.1791 Min: 0.1338 \n",
      "Global iteration 42\n",
      "RMSE Mean: 0.1561 Std: 0.0156 Max: 0.1797 Min: 0.1338 \n",
      "Global iteration 43\n",
      "RMSE Mean: 0.1558 Std: 0.0154 Max: 0.1798 Min: 0.1349 \n",
      "Global iteration 44\n",
      "RMSE Mean: 0.1557 Std: 0.0157 Max: 0.1802 Min: 0.1345 \n",
      "Global iteration 45\n",
      "RMSE Mean: 0.1552 Std: 0.0157 Max: 0.1794 Min: 0.1338 \n",
      "Global iteration 46\n",
      "RMSE Mean: 0.1555 Std: 0.0157 Max: 0.1798 Min: 0.1332 \n",
      "Global iteration 47\n",
      "RMSE Mean: 0.1548 Std: 0.0156 Max: 0.1795 Min: 0.1334 \n",
      "Global iteration 48\n",
      "RMSE Mean: 0.1553 Std: 0.0153 Max: 0.1805 Min: 0.1342 \n",
      "Global iteration 49\n",
      "RMSE Mean: 0.1554 Std: 0.0156 Max: 0.1793 Min: 0.1340 \n"
     ]
    }
   ],
   "source": [
    "global_iteration = 50\n",
    "\n",
    "train_params = {\n",
    "    'lr': 1e-3,\n",
    "    'weight_decay': 1e-5,\n",
    "    'epochs': 5,\n",
    "    'batch_size': 64,\n",
    "    'verbose': 10\n",
    "}\n",
    "\n",
    "for it in range(global_iteration):\n",
    "    print(f\"Global iteration {it}\")\n",
    "    \n",
    "    # local model training\n",
    "    params = []\n",
    "    for client in clients:\n",
    "        params.append(\n",
    "            local_train(client.imp_model, client.X_imp, client.mask, train_params, client.X, return_params=True)\n",
    "        )\n",
    "    \n",
    "    fit_res = [{'sample_size': client.X.shape[0]} for client in clients]\n",
    "    \n",
    "    # federated averaging\n",
    "    avg_params, _ = fedavg(params, fit_res)\n",
    "    \n",
    "    for idx, client in enumerate(clients):\n",
    "        client.imp_model.load_state_dict(avg_params[idx])\n",
    "        \n",
    "    # evaluation\n",
    "    evaluate_model(clients)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FedAvg FT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "clients = [Client(X[i].copy(), X_ms[i].copy(), y[i].copy()) for i in range(len(X))]\n",
    "for idx, client in enumerate(clients):\n",
    "    client.imp_model = MIWAE(\n",
    "        client.X.shape[1], latent_size=10, n_hidden=32, n_hidden_layers=2, seed=idx, out_dist='gaussian', K=20, L=100\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global iteration 0\n",
      "RMSE Mean: 0.2570 Std: 0.0159 Max: 0.2834 Min: 0.2242 \n",
      "Global iteration 1\n",
      "RMSE Mean: 0.1726 Std: 0.0207 Max: 0.1984 Min: 0.1219 \n",
      "Global iteration 2\n",
      "RMSE Mean: 0.1708 Std: 0.0211 Max: 0.1973 Min: 0.1200 \n",
      "Global iteration 3\n",
      "RMSE Mean: 0.1713 Std: 0.0214 Max: 0.1980 Min: 0.1187 \n",
      "Global iteration 4\n",
      "RMSE Mean: 0.1714 Std: 0.0221 Max: 0.1976 Min: 0.1177 \n",
      "Global iteration 5\n",
      "RMSE Mean: 0.1715 Std: 0.0222 Max: 0.1992 Min: 0.1175 \n",
      "Global iteration 6\n",
      "RMSE Mean: 0.1717 Std: 0.0222 Max: 0.1985 Min: 0.1175 \n",
      "Global iteration 7\n",
      "RMSE Mean: 0.1716 Std: 0.0219 Max: 0.1978 Min: 0.1184 \n",
      "Global iteration 8\n",
      "RMSE Mean: 0.1722 Std: 0.0227 Max: 0.2001 Min: 0.1176 \n",
      "Global iteration 9\n",
      "RMSE Mean: 0.1732 Std: 0.0237 Max: 0.2028 Min: 0.1163 \n",
      "Global iteration 10\n",
      "RMSE Mean: 0.1733 Std: 0.0235 Max: 0.2027 Min: 0.1172 \n",
      "Global iteration 11\n",
      "RMSE Mean: 0.1742 Std: 0.0240 Max: 0.2043 Min: 0.1171 \n",
      "Global iteration 12\n",
      "RMSE Mean: 0.1747 Std: 0.0245 Max: 0.2055 Min: 0.1164 \n",
      "Global iteration 13\n",
      "RMSE Mean: 0.1750 Std: 0.0251 Max: 0.2066 Min: 0.1158 \n",
      "Global iteration 14\n",
      "RMSE Mean: 0.1760 Std: 0.0264 Max: 0.2086 Min: 0.1144 \n",
      "Global iteration 15\n",
      "RMSE Mean: 0.1767 Std: 0.0269 Max: 0.2094 Min: 0.1143 \n",
      "Global iteration 16\n",
      "RMSE Mean: 0.1762 Std: 0.0275 Max: 0.2117 Min: 0.1139 \n",
      "Global iteration 17\n",
      "RMSE Mean: 0.1768 Std: 0.0278 Max: 0.2116 Min: 0.1142 \n",
      "Global iteration 18\n",
      "RMSE Mean: 0.1768 Std: 0.0285 Max: 0.2140 Min: 0.1129 \n",
      "Global iteration 19\n",
      "RMSE Mean: 0.1756 Std: 0.0275 Max: 0.2102 Min: 0.1136 \n",
      "Global iteration 20\n",
      "RMSE Mean: 0.1766 Std: 0.0286 Max: 0.2123 Min: 0.1134 \n",
      "Global iteration 21\n",
      "RMSE Mean: 0.1759 Std: 0.0281 Max: 0.2111 Min: 0.1139 \n",
      "Global iteration 22\n",
      "RMSE Mean: 0.1762 Std: 0.0288 Max: 0.2131 Min: 0.1138 \n",
      "Global iteration 23\n",
      "RMSE Mean: 0.1756 Std: 0.0280 Max: 0.2088 Min: 0.1135 \n",
      "Global iteration 24\n",
      "RMSE Mean: 0.1762 Std: 0.0290 Max: 0.2132 Min: 0.1135 \n",
      "Global iteration 25\n",
      "RMSE Mean: 0.1766 Std: 0.0292 Max: 0.2153 Min: 0.1137 \n",
      "Global iteration 26\n",
      "RMSE Mean: 0.1763 Std: 0.0292 Max: 0.2128 Min: 0.1129 \n",
      "Global iteration 27\n",
      "RMSE Mean: 0.1762 Std: 0.0291 Max: 0.2140 Min: 0.1139 \n",
      "Global iteration 28\n",
      "RMSE Mean: 0.1761 Std: 0.0285 Max: 0.2135 Min: 0.1139 \n",
      "Global iteration 29\n",
      "RMSE Mean: 0.1762 Std: 0.0292 Max: 0.2136 Min: 0.1136 \n",
      "Global iteration 30\n",
      "RMSE Mean: 0.1754 Std: 0.0290 Max: 0.2120 Min: 0.1125 \n",
      "Global iteration 31\n",
      "RMSE Mean: 0.1754 Std: 0.0285 Max: 0.2114 Min: 0.1137 \n",
      "Global iteration 32\n",
      "RMSE Mean: 0.1761 Std: 0.0291 Max: 0.2133 Min: 0.1129 \n",
      "Global iteration 33\n",
      "RMSE Mean: 0.1760 Std: 0.0290 Max: 0.2145 Min: 0.1133 \n",
      "Global iteration 34\n",
      "RMSE Mean: 0.1754 Std: 0.0289 Max: 0.2119 Min: 0.1131 \n",
      "Global iteration 35\n",
      "RMSE Mean: 0.1760 Std: 0.0287 Max: 0.2137 Min: 0.1128 \n",
      "Global iteration 36\n",
      "RMSE Mean: 0.1755 Std: 0.0292 Max: 0.2138 Min: 0.1127 \n",
      "Global iteration 37\n",
      "RMSE Mean: 0.1758 Std: 0.0289 Max: 0.2117 Min: 0.1134 \n",
      "Global iteration 38\n",
      "RMSE Mean: 0.1751 Std: 0.0284 Max: 0.2119 Min: 0.1129 \n",
      "Global iteration 39\n",
      "RMSE Mean: 0.1757 Std: 0.0284 Max: 0.2130 Min: 0.1145 \n",
      "Global iteration 40\n",
      "RMSE Mean: 0.1753 Std: 0.0284 Max: 0.2117 Min: 0.1141 \n",
      "Global iteration 41\n",
      "RMSE Mean: 0.1744 Std: 0.0287 Max: 0.2110 Min: 0.1128 \n",
      "Global iteration 42\n",
      "RMSE Mean: 0.1749 Std: 0.0284 Max: 0.2117 Min: 0.1138 \n",
      "Global iteration 43\n",
      "RMSE Mean: 0.1744 Std: 0.0283 Max: 0.2122 Min: 0.1136 \n",
      "Global iteration 44\n",
      "RMSE Mean: 0.1745 Std: 0.0286 Max: 0.2120 Min: 0.1130 \n",
      "Global iteration 45\n",
      "RMSE Mean: 0.1745 Std: 0.0280 Max: 0.2107 Min: 0.1137 \n",
      "Global iteration 46\n",
      "RMSE Mean: 0.1749 Std: 0.0283 Max: 0.2113 Min: 0.1136 \n",
      "Global iteration 47\n",
      "RMSE Mean: 0.1748 Std: 0.0276 Max: 0.2085 Min: 0.1149 \n",
      "Global iteration 48\n",
      "RMSE Mean: 0.1752 Std: 0.0283 Max: 0.2109 Min: 0.1140 \n",
      "Global iteration 49\n",
      "RMSE Mean: 0.1751 Std: 0.0283 Max: 0.2115 Min: 0.1143 \n",
      "Global iteration 50\n",
      "RMSE Mean: 0.1744 Std: 0.0273 Max: 0.2092 Min: 0.1145 \n",
      "Global iteration 51\n",
      "RMSE Mean: 0.1742 Std: 0.0277 Max: 0.2092 Min: 0.1136 \n",
      "Global iteration 52\n",
      "RMSE Mean: 0.1751 Std: 0.0280 Max: 0.2113 Min: 0.1144 \n",
      "Global iteration 53\n",
      "RMSE Mean: 0.1744 Std: 0.0279 Max: 0.2119 Min: 0.1144 \n",
      "Global iteration 54\n",
      "RMSE Mean: 0.1740 Std: 0.0280 Max: 0.2101 Min: 0.1131 \n",
      "Global iteration 55\n",
      "RMSE Mean: 0.1750 Std: 0.0279 Max: 0.2118 Min: 0.1147 \n",
      "Global iteration 56\n",
      "RMSE Mean: 0.1744 Std: 0.0277 Max: 0.2117 Min: 0.1147 \n",
      "Global iteration 57\n",
      "RMSE Mean: 0.1745 Std: 0.0284 Max: 0.2120 Min: 0.1140 \n",
      "Global iteration 58\n",
      "RMSE Mean: 0.1744 Std: 0.0279 Max: 0.2097 Min: 0.1143 \n",
      "Global iteration 59\n",
      "RMSE Mean: 0.1744 Std: 0.0279 Max: 0.2117 Min: 0.1143 \n",
      "Fine-tuning step 0\n",
      "RMSE Mean: 0.1602 Std: 0.0258 Max: 0.2050 Min: 0.1035 \n",
      "Fine-tuning step 1\n",
      "RMSE Mean: 0.1566 Std: 0.0229 Max: 0.1959 Min: 0.1037 \n",
      "Fine-tuning step 2\n",
      "RMSE Mean: 0.1537 Std: 0.0218 Max: 0.1937 Min: 0.1039 \n",
      "Fine-tuning step 3\n",
      "RMSE Mean: 0.1521 Std: 0.0203 Max: 0.1838 Min: 0.1028 \n",
      "Fine-tuning step 4\n",
      "RMSE Mean: 0.1509 Std: 0.0203 Max: 0.1760 Min: 0.1023 \n",
      "Fine-tuning step 5\n",
      "RMSE Mean: 0.1480 Std: 0.0189 Max: 0.1702 Min: 0.1014 \n",
      "Fine-tuning step 6\n",
      "RMSE Mean: 0.1483 Std: 0.0193 Max: 0.1720 Min: 0.1014 \n",
      "Fine-tuning step 7\n",
      "RMSE Mean: 0.1487 Std: 0.0193 Max: 0.1719 Min: 0.1016 \n",
      "Fine-tuning step 8\n",
      "RMSE Mean: 0.1469 Std: 0.0184 Max: 0.1697 Min: 0.1011 \n",
      "Fine-tuning step 9\n",
      "RMSE Mean: 0.1467 Std: 0.0187 Max: 0.1706 Min: 0.0999 \n",
      "Fine-tuning step 10\n",
      "RMSE Mean: 0.1456 Std: 0.0174 Max: 0.1660 Min: 0.1008 \n",
      "Fine-tuning step 11\n",
      "RMSE Mean: 0.1461 Std: 0.0178 Max: 0.1680 Min: 0.1005 \n",
      "Fine-tuning step 12\n",
      "RMSE Mean: 0.1453 Std: 0.0177 Max: 0.1691 Min: 0.1000 \n",
      "Fine-tuning step 13\n",
      "RMSE Mean: 0.1451 Std: 0.0173 Max: 0.1690 Min: 0.1010 \n",
      "Fine-tuning step 14\n",
      "RMSE Mean: 0.1457 Std: 0.0175 Max: 0.1667 Min: 0.1006 \n",
      "Fine-tuning step 15\n",
      "RMSE Mean: 0.1445 Std: 0.0174 Max: 0.1664 Min: 0.1008 \n",
      "Fine-tuning step 16\n",
      "RMSE Mean: 0.1463 Std: 0.0180 Max: 0.1670 Min: 0.0997 \n",
      "Fine-tuning step 17\n",
      "RMSE Mean: 0.1467 Std: 0.0179 Max: 0.1699 Min: 0.1010 \n",
      "Fine-tuning step 18\n",
      "RMSE Mean: 0.1454 Std: 0.0175 Max: 0.1686 Min: 0.1009 \n",
      "Fine-tuning step 19\n",
      "RMSE Mean: 0.1451 Std: 0.0175 Max: 0.1691 Min: 0.1005 \n",
      "Fine-tuning step 20\n",
      "RMSE Mean: 0.1459 Std: 0.0177 Max: 0.1672 Min: 0.1003 \n",
      "Fine-tuning step 21\n",
      "RMSE Mean: 0.1455 Std: 0.0170 Max: 0.1677 Min: 0.1016 \n",
      "Fine-tuning step 22\n",
      "RMSE Mean: 0.1451 Std: 0.0170 Max: 0.1667 Min: 0.1010 \n",
      "Fine-tuning step 23\n",
      "RMSE Mean: 0.1445 Std: 0.0169 Max: 0.1655 Min: 0.1005 \n",
      "Fine-tuning step 24\n",
      "RMSE Mean: 0.1457 Std: 0.0170 Max: 0.1654 Min: 0.1010 \n",
      "Fine-tuning step 25\n",
      "RMSE Mean: 0.1454 Std: 0.0170 Max: 0.1664 Min: 0.1018 \n",
      "Fine-tuning step 26\n",
      "RMSE Mean: 0.1457 Std: 0.0170 Max: 0.1659 Min: 0.1020 \n",
      "Fine-tuning step 27\n",
      "RMSE Mean: 0.1441 Std: 0.0162 Max: 0.1622 Min: 0.1017 \n",
      "Fine-tuning step 28\n",
      "RMSE Mean: 0.1444 Std: 0.0171 Max: 0.1650 Min: 0.1008 \n",
      "Fine-tuning step 29\n",
      "RMSE Mean: 0.1456 Std: 0.0167 Max: 0.1651 Min: 0.1026 \n",
      "Fine-tuning step 30\n",
      "RMSE Mean: 0.1465 Std: 0.0174 Max: 0.1639 Min: 0.1022 \n",
      "Fine-tuning step 31\n",
      "RMSE Mean: 0.1451 Std: 0.0169 Max: 0.1662 Min: 0.1011 \n",
      "Fine-tuning step 32\n",
      "RMSE Mean: 0.1445 Std: 0.0166 Max: 0.1634 Min: 0.1008 \n",
      "Fine-tuning step 33\n",
      "RMSE Mean: 0.1449 Std: 0.0164 Max: 0.1634 Min: 0.1016 \n",
      "Fine-tuning step 34\n",
      "RMSE Mean: 0.1439 Std: 0.0167 Max: 0.1640 Min: 0.1011 \n",
      "Fine-tuning step 35\n",
      "RMSE Mean: 0.1448 Std: 0.0167 Max: 0.1641 Min: 0.1004 \n",
      "Fine-tuning step 36\n",
      "RMSE Mean: 0.1449 Std: 0.0166 Max: 0.1622 Min: 0.1018 \n",
      "Fine-tuning step 37\n",
      "RMSE Mean: 0.1446 Std: 0.0168 Max: 0.1632 Min: 0.1014 \n",
      "Fine-tuning step 38\n",
      "RMSE Mean: 0.1449 Std: 0.0166 Max: 0.1641 Min: 0.1022 \n",
      "Fine-tuning step 39\n",
      "RMSE Mean: 0.1443 Std: 0.0163 Max: 0.1616 Min: 0.1005 \n"
     ]
    }
   ],
   "source": [
    "global_iteration = 60\n",
    "num_ft_steps = 40\n",
    "\n",
    "train_params = {\n",
    "    'lr': 1e-3,\n",
    "    'weight_decay': 1e-5,\n",
    "    'epochs': 5,\n",
    "    'batch_size': 64,\n",
    "    'verbose': 10\n",
    "}\n",
    "\n",
    "for it in range(global_iteration):\n",
    "    print(f\"Global iteration {it}\")\n",
    "    \n",
    "    # local model training\n",
    "    params = []\n",
    "    for client in clients:\n",
    "        params.append(\n",
    "            local_train(client.imp_model, client.X_imp, client.mask, train_params, client.X, return_params=True)\n",
    "        )\n",
    "    \n",
    "    fit_res = [{'sample_size': client.X.shape[0]} for client in clients]\n",
    "    \n",
    "    # federated averaging\n",
    "    avg_params, _ = fedavg(params, fit_res)\n",
    "    \n",
    "    for idx, client in enumerate(clients):\n",
    "        client.imp_model.load_state_dict(avg_params[idx])\n",
    "        \n",
    "    # evaluation\n",
    "    evaluate_model(clients)\n",
    "\n",
    "for it in range(num_ft_steps):\n",
    "    print(f\"Fine-tuning step {it}\")\n",
    "    \n",
    "    # local model training\n",
    "    params = []\n",
    "    for client in clients:\n",
    "        params.append(\n",
    "            local_train(client.imp_model, client.X_imp, client.mask, train_params, client.X, return_params=True)\n",
    "        )\n",
    "        \n",
    "    # evaluation\n",
    "    evaluate_model(clients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FedProx and FT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Optimizer\n",
    "import copy\n",
    "\n",
    "class FedProxOptimizer(Optimizer):\n",
    "    def __init__(self, params, lr=0.01, lamda=0.1, mu=0.001):\n",
    "        if lr < 0.0:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        defaults=dict(lr=lr, lamda=lamda, mu=mu)\n",
    "        super(FedProxOptimizer, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self, vstar, closure=None):\n",
    "        loss=None\n",
    "        if closure is not None:\n",
    "            loss=closure\n",
    "        for group in self.param_groups:\n",
    "            for p, pstar in zip(group['params'], vstar):\n",
    "                # w <=== w - lr * ( w'  + lambda * (w - w* ) + mu * w )\n",
    "                p.data=p.data - group['lr'] * (\n",
    "                            p.grad.data + group['lamda'] * (p.data - pstar.data.clone()) + group['mu'] * p.data)\n",
    "        return group['params'], loss\n",
    "\n",
    "def local_train_prox(model, X, mask, train_params, X_true, return_params = False, mu = 0.001):\n",
    "    \n",
    "    global_model = MIWAE(\n",
    "        X.shape[1], latent_size=10, n_hidden=32, n_hidden_layers=2, seed=0, out_dist='gaussian', K=20, L=100\n",
    "    )\n",
    "    global_model.load_state_dict(copy.deepcopy(model.state_dict()))\n",
    "    model.to(DEVICE)\n",
    "    global_model.to(DEVICE)\n",
    "    lr = train_params['lr']\n",
    "    weight_decay = train_params['weight_decay']\n",
    "    epochs = train_params['epochs']\n",
    "    batch_size = train_params['batch_size']\n",
    "    verbose = train_params['verbose']\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay, amsgrad=True)\n",
    "\n",
    "    # data\n",
    "    n = X.shape[0]\n",
    "    X_imp = X.copy()\n",
    "    X_mask = mask.copy()\n",
    "    bs = min(batch_size, n)\n",
    "\n",
    "    final_loss = 0\n",
    "    rmses = []\n",
    "    for ep in range(epochs):\n",
    "        \n",
    "        # evaluation\n",
    "        with torch.no_grad():\n",
    "            X_imp_new = model.impute(\n",
    "                torch.from_numpy(X_imp).float().to(DEVICE), torch.from_numpy(~X_mask).float().to(DEVICE)\n",
    "            )\n",
    "            rmse_value = rmse(X_imp_new.detach().clone().cpu().numpy(), X_true, X_mask)\n",
    "            rmses.append(rmse_value)\n",
    "\n",
    "        # shuffle data\n",
    "        perm = np.random.permutation(n)  # We use the \"random reshuffling\" version of SGD\n",
    "        batches_data = np.array_split(X_imp[perm,], int(n / bs), )\n",
    "        batches_mask = np.array_split(X_mask[perm,], int(n / bs), )\n",
    "        total_loss, total_iters = 0, 0\n",
    "        total_mask_loss = 0\n",
    "        model.train()\n",
    "        for it in range(len(batches_data)):\n",
    "            optimizer.zero_grad()\n",
    "            model.encoder.zero_grad()\n",
    "            model.decoder.zero_grad()\n",
    "            model.mask_net.zero_grad()\n",
    "            b_data = torch.from_numpy(batches_data[it]).float().to(DEVICE)\n",
    "            b_mask = torch.from_numpy(~batches_mask[it]).float().to(DEVICE)\n",
    "            data = [b_data, b_mask]\n",
    "            loss, ret_dict = model.compute_loss(data)\n",
    "            proximal_term = 0.0\n",
    "            for w, w_t in zip(model.parameters(), global_model.parameters()):\n",
    "                proximal_term += (w - w_t).norm(2)\n",
    "            loss = loss + (mu / 2) * proximal_term\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_iters += 1\n",
    "\n",
    "\n",
    "        # print loss\n",
    "        if (ep + 1) % verbose == 0:\n",
    "            print('Epoch %s/%s, Loss = %s RMSE = %s' % (ep, epochs, total_loss / total_iters, rmses[-1]))\n",
    "\n",
    "        if DEVICE == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        final_loss = total_loss / total_iters\n",
    "\n",
    "    model.to(\"cpu\")\n",
    "    global_model.to(\"cpu\")\n",
    "    del global_model\n",
    "    \n",
    "    if return_params:\n",
    "        return deepcopy(model.state_dict())\n",
    "    else:\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "clients = [Client(X[i].copy(), X_ms[i].copy(), y[i].copy()) for i in range(len(X))]\n",
    "for idx, client in enumerate(clients):\n",
    "    client.imp_model = MIWAE(\n",
    "        client.X.shape[1], latent_size=10, n_hidden=32, n_hidden_layers=2, seed=idx, out_dist='gaussian', K=20, L=100\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global iteration 0\n",
      "RMSE Mean: 0.2542 Std: 0.0170 Max: 0.2805 Min: 0.2212 \n",
      "Global iteration 1\n",
      "RMSE Mean: 0.1721 Std: 0.0200 Max: 0.1966 Min: 0.1232 \n",
      "Global iteration 2\n",
      "RMSE Mean: 0.1715 Std: 0.0208 Max: 0.1967 Min: 0.1205 \n",
      "Global iteration 3\n",
      "RMSE Mean: 0.1711 Std: 0.0211 Max: 0.1968 Min: 0.1196 \n",
      "Global iteration 4\n",
      "RMSE Mean: 0.1712 Std: 0.0219 Max: 0.1979 Min: 0.1183 \n",
      "Global iteration 5\n",
      "RMSE Mean: 0.1714 Std: 0.0221 Max: 0.1986 Min: 0.1181 \n",
      "Global iteration 6\n",
      "RMSE Mean: 0.1715 Std: 0.0219 Max: 0.1982 Min: 0.1185 \n",
      "Global iteration 7\n",
      "RMSE Mean: 0.1719 Std: 0.0221 Max: 0.1989 Min: 0.1186 \n",
      "Global iteration 8\n",
      "RMSE Mean: 0.1724 Std: 0.0226 Max: 0.2004 Min: 0.1182 \n",
      "Global iteration 9\n",
      "RMSE Mean: 0.1731 Std: 0.0233 Max: 0.2023 Min: 0.1175 \n",
      "Global iteration 10\n",
      "RMSE Mean: 0.1737 Std: 0.0238 Max: 0.2036 Min: 0.1168 \n",
      "Global iteration 11\n",
      "RMSE Mean: 0.1743 Std: 0.0243 Max: 0.2046 Min: 0.1163 \n",
      "Global iteration 12\n",
      "RMSE Mean: 0.1750 Std: 0.0249 Max: 0.2058 Min: 0.1156 \n",
      "Global iteration 13\n",
      "RMSE Mean: 0.1755 Std: 0.0256 Max: 0.2074 Min: 0.1149 \n",
      "Global iteration 14\n",
      "RMSE Mean: 0.1759 Std: 0.0264 Max: 0.2090 Min: 0.1142 \n",
      "Global iteration 15\n",
      "RMSE Mean: 0.1760 Std: 0.0269 Max: 0.2100 Min: 0.1137 \n",
      "Global iteration 16\n",
      "RMSE Mean: 0.1760 Std: 0.0272 Max: 0.2105 Min: 0.1134 \n",
      "Global iteration 17\n",
      "RMSE Mean: 0.1759 Std: 0.0273 Max: 0.2107 Min: 0.1131 \n",
      "Global iteration 18\n",
      "RMSE Mean: 0.1758 Std: 0.0275 Max: 0.2108 Min: 0.1128 \n",
      "Global iteration 19\n",
      "RMSE Mean: 0.1760 Std: 0.0277 Max: 0.2114 Min: 0.1130 \n",
      "Global iteration 20\n",
      "RMSE Mean: 0.1761 Std: 0.0283 Max: 0.2128 Min: 0.1131 \n",
      "Global iteration 21\n",
      "RMSE Mean: 0.1762 Std: 0.0286 Max: 0.2135 Min: 0.1133 \n",
      "Global iteration 22\n",
      "RMSE Mean: 0.1762 Std: 0.0287 Max: 0.2138 Min: 0.1134 \n",
      "Global iteration 23\n",
      "RMSE Mean: 0.1762 Std: 0.0288 Max: 0.2138 Min: 0.1134 \n",
      "Global iteration 24\n",
      "RMSE Mean: 0.1762 Std: 0.0288 Max: 0.2140 Min: 0.1135 \n",
      "Global iteration 25\n",
      "RMSE Mean: 0.1763 Std: 0.0290 Max: 0.2143 Min: 0.1135 \n",
      "Global iteration 26\n",
      "RMSE Mean: 0.1764 Std: 0.0291 Max: 0.2146 Min: 0.1135 \n",
      "Global iteration 27\n",
      "RMSE Mean: 0.1765 Std: 0.0292 Max: 0.2148 Min: 0.1135 \n",
      "Global iteration 28\n",
      "RMSE Mean: 0.1766 Std: 0.0292 Max: 0.2148 Min: 0.1134 \n",
      "Global iteration 29\n",
      "RMSE Mean: 0.1766 Std: 0.0292 Max: 0.2148 Min: 0.1133 \n",
      "Global iteration 30\n",
      "RMSE Mean: 0.1766 Std: 0.0292 Max: 0.2148 Min: 0.1133 \n",
      "Global iteration 31\n",
      "RMSE Mean: 0.1766 Std: 0.0293 Max: 0.2148 Min: 0.1132 \n",
      "Global iteration 32\n",
      "RMSE Mean: 0.1766 Std: 0.0293 Max: 0.2147 Min: 0.1131 \n",
      "Global iteration 33\n",
      "RMSE Mean: 0.1766 Std: 0.0293 Max: 0.2146 Min: 0.1130 \n",
      "Global iteration 34\n",
      "RMSE Mean: 0.1766 Std: 0.0293 Max: 0.2145 Min: 0.1129 \n",
      "Global iteration 35\n",
      "RMSE Mean: 0.1765 Std: 0.0293 Max: 0.2144 Min: 0.1129 \n",
      "Global iteration 36\n",
      "RMSE Mean: 0.1765 Std: 0.0293 Max: 0.2143 Min: 0.1128 \n",
      "Global iteration 37\n",
      "RMSE Mean: 0.1764 Std: 0.0292 Max: 0.2142 Min: 0.1128 \n",
      "Global iteration 38\n",
      "RMSE Mean: 0.1764 Std: 0.0292 Max: 0.2142 Min: 0.1128 \n",
      "Global iteration 39\n",
      "RMSE Mean: 0.1763 Std: 0.0293 Max: 0.2141 Min: 0.1127 \n",
      "Global iteration 40\n",
      "RMSE Mean: 0.1763 Std: 0.0293 Max: 0.2141 Min: 0.1127 \n",
      "Global iteration 41\n",
      "RMSE Mean: 0.1763 Std: 0.0293 Max: 0.2140 Min: 0.1126 \n",
      "Global iteration 42\n",
      "RMSE Mean: 0.1763 Std: 0.0293 Max: 0.2141 Min: 0.1126 \n",
      "Global iteration 43\n",
      "RMSE Mean: 0.1763 Std: 0.0293 Max: 0.2140 Min: 0.1126 \n",
      "Global iteration 44\n",
      "RMSE Mean: 0.1763 Std: 0.0293 Max: 0.2140 Min: 0.1126 \n",
      "Global iteration 45\n",
      "RMSE Mean: 0.1764 Std: 0.0293 Max: 0.2140 Min: 0.1127 \n",
      "Global iteration 46\n",
      "RMSE Mean: 0.1764 Std: 0.0293 Max: 0.2140 Min: 0.1127 \n",
      "Global iteration 47\n",
      "RMSE Mean: 0.1764 Std: 0.0294 Max: 0.2141 Min: 0.1127 \n",
      "Global iteration 48\n",
      "RMSE Mean: 0.1763 Std: 0.0294 Max: 0.2140 Min: 0.1127 \n",
      "Global iteration 49\n",
      "RMSE Mean: 0.1763 Std: 0.0294 Max: 0.2140 Min: 0.1127 \n",
      "Fine-tuning step 0\n",
      "RMSE Mean: 0.1617 Std: 0.0264 Max: 0.2068 Min: 0.1035 \n",
      "Fine-tuning step 1\n",
      "RMSE Mean: 0.1578 Std: 0.0234 Max: 0.1920 Min: 0.1029 \n",
      "Fine-tuning step 2\n",
      "RMSE Mean: 0.1555 Std: 0.0227 Max: 0.1901 Min: 0.1021 \n",
      "Fine-tuning step 3\n",
      "RMSE Mean: 0.1522 Std: 0.0209 Max: 0.1809 Min: 0.1011 \n",
      "Fine-tuning step 4\n",
      "RMSE Mean: 0.1507 Std: 0.0190 Max: 0.1718 Min: 0.1024 \n",
      "Fine-tuning step 5\n",
      "RMSE Mean: 0.1487 Std: 0.0191 Max: 0.1700 Min: 0.1006 \n",
      "Fine-tuning step 6\n",
      "RMSE Mean: 0.1489 Std: 0.0184 Max: 0.1696 Min: 0.1011 \n",
      "Fine-tuning step 7\n",
      "RMSE Mean: 0.1490 Std: 0.0189 Max: 0.1723 Min: 0.1014 \n",
      "Fine-tuning step 8\n",
      "RMSE Mean: 0.1479 Std: 0.0185 Max: 0.1702 Min: 0.1002 \n",
      "Fine-tuning step 9\n",
      "RMSE Mean: 0.1473 Std: 0.0186 Max: 0.1704 Min: 0.1005 \n",
      "Fine-tuning step 10\n",
      "RMSE Mean: 0.1468 Std: 0.0180 Max: 0.1702 Min: 0.1007 \n",
      "Fine-tuning step 11\n",
      "RMSE Mean: 0.1465 Std: 0.0176 Max: 0.1688 Min: 0.1022 \n",
      "Fine-tuning step 12\n",
      "RMSE Mean: 0.1468 Std: 0.0181 Max: 0.1683 Min: 0.1013 \n",
      "Fine-tuning step 13\n",
      "RMSE Mean: 0.1461 Std: 0.0175 Max: 0.1677 Min: 0.1008 \n",
      "Fine-tuning step 14\n",
      "RMSE Mean: 0.1467 Std: 0.0179 Max: 0.1657 Min: 0.1013 \n",
      "Fine-tuning step 15\n",
      "RMSE Mean: 0.1459 Std: 0.0177 Max: 0.1696 Min: 0.1010 \n",
      "Fine-tuning step 16\n",
      "RMSE Mean: 0.1463 Std: 0.0179 Max: 0.1690 Min: 0.1010 \n",
      "Fine-tuning step 17\n",
      "RMSE Mean: 0.1458 Std: 0.0177 Max: 0.1672 Min: 0.1003 \n",
      "Fine-tuning step 18\n",
      "RMSE Mean: 0.1456 Std: 0.0177 Max: 0.1677 Min: 0.1007 \n",
      "Fine-tuning step 19\n",
      "RMSE Mean: 0.1463 Std: 0.0178 Max: 0.1675 Min: 0.1004 \n",
      "Fine-tuning step 20\n",
      "RMSE Mean: 0.1456 Std: 0.0168 Max: 0.1655 Min: 0.1014 \n",
      "Fine-tuning step 21\n",
      "RMSE Mean: 0.1465 Std: 0.0173 Max: 0.1667 Min: 0.1020 \n",
      "Fine-tuning step 22\n",
      "RMSE Mean: 0.1454 Std: 0.0177 Max: 0.1658 Min: 0.1002 \n",
      "Fine-tuning step 23\n",
      "RMSE Mean: 0.1459 Std: 0.0169 Max: 0.1654 Min: 0.1008 \n",
      "Fine-tuning step 24\n",
      "RMSE Mean: 0.1458 Std: 0.0172 Max: 0.1660 Min: 0.1009 \n",
      "Fine-tuning step 25\n",
      "RMSE Mean: 0.1464 Std: 0.0172 Max: 0.1655 Min: 0.1008 \n",
      "Fine-tuning step 26\n",
      "RMSE Mean: 0.1454 Std: 0.0171 Max: 0.1652 Min: 0.1014 \n",
      "Fine-tuning step 27\n",
      "RMSE Mean: 0.1458 Std: 0.0176 Max: 0.1659 Min: 0.1005 \n",
      "Fine-tuning step 28\n",
      "RMSE Mean: 0.1457 Std: 0.0177 Max: 0.1648 Min: 0.1010 \n",
      "Fine-tuning step 29\n",
      "RMSE Mean: 0.1457 Std: 0.0174 Max: 0.1655 Min: 0.1005 \n",
      "Fine-tuning step 30\n",
      "RMSE Mean: 0.1455 Std: 0.0175 Max: 0.1687 Min: 0.1001 \n",
      "Fine-tuning step 31\n",
      "RMSE Mean: 0.1450 Std: 0.0175 Max: 0.1649 Min: 0.1001 \n",
      "Fine-tuning step 32\n",
      "RMSE Mean: 0.1445 Std: 0.0168 Max: 0.1650 Min: 0.1002 \n",
      "Fine-tuning step 33\n",
      "RMSE Mean: 0.1448 Std: 0.0166 Max: 0.1643 Min: 0.1004 \n",
      "Fine-tuning step 34\n",
      "RMSE Mean: 0.1449 Std: 0.0175 Max: 0.1660 Min: 0.1004 \n",
      "Fine-tuning step 35\n",
      "RMSE Mean: 0.1455 Std: 0.0170 Max: 0.1657 Min: 0.1013 \n",
      "Fine-tuning step 36\n",
      "RMSE Mean: 0.1456 Std: 0.0171 Max: 0.1640 Min: 0.1006 \n",
      "Fine-tuning step 37\n",
      "RMSE Mean: 0.1446 Std: 0.0171 Max: 0.1672 Min: 0.1010 \n",
      "Fine-tuning step 38\n",
      "RMSE Mean: 0.1446 Std: 0.0165 Max: 0.1635 Min: 0.1011 \n",
      "Fine-tuning step 39\n",
      "RMSE Mean: 0.1447 Std: 0.0170 Max: 0.1640 Min: 0.1006 \n",
      "Fine-tuning step 40\n",
      "RMSE Mean: 0.1445 Std: 0.0169 Max: 0.1640 Min: 0.0998 \n",
      "Fine-tuning step 41\n",
      "RMSE Mean: 0.1445 Std: 0.0165 Max: 0.1647 Min: 0.1020 \n",
      "Fine-tuning step 42\n",
      "RMSE Mean: 0.1442 Std: 0.0164 Max: 0.1645 Min: 0.1012 \n",
      "Fine-tuning step 43\n",
      "RMSE Mean: 0.1453 Std: 0.0171 Max: 0.1622 Min: 0.1010 \n",
      "Fine-tuning step 44\n",
      "RMSE Mean: 0.1451 Std: 0.0167 Max: 0.1631 Min: 0.1020 \n",
      "Fine-tuning step 45\n",
      "RMSE Mean: 0.1447 Std: 0.0168 Max: 0.1634 Min: 0.0998 \n",
      "Fine-tuning step 46\n",
      "RMSE Mean: 0.1446 Std: 0.0170 Max: 0.1639 Min: 0.1014 \n",
      "Fine-tuning step 47\n",
      "RMSE Mean: 0.1450 Std: 0.0170 Max: 0.1616 Min: 0.1008 \n",
      "Fine-tuning step 48\n",
      "RMSE Mean: 0.1448 Std: 0.0168 Max: 0.1640 Min: 0.1018 \n",
      "Fine-tuning step 49\n",
      "RMSE Mean: 0.1444 Std: 0.0165 Max: 0.1617 Min: 0.1007 \n"
     ]
    }
   ],
   "source": [
    "global_iteration = 50\n",
    "num_ft_steps = 50\n",
    "\n",
    "train_params = {\n",
    "    'lr': 1e-3,\n",
    "    'weight_decay': 1e-5,\n",
    "    'epochs': 5,\n",
    "    'batch_size': 64,\n",
    "    'verbose': 10\n",
    "}\n",
    "\n",
    "for it in range(global_iteration):\n",
    "    print(f\"Global iteration {it}\")\n",
    "    \n",
    "    # local model training\n",
    "    params = []\n",
    "    for client in clients:\n",
    "        params.append(\n",
    "            local_train_prox(client.imp_model, client.X_imp, client.mask, train_params, client.X, return_params=True)\n",
    "        )\n",
    "    \n",
    "    fit_res = [{'sample_size': client.X.shape[0]} for client in clients]\n",
    "    \n",
    "    # federated averaging\n",
    "    avg_params, _ = fedavg(params, fit_res)\n",
    "    \n",
    "    for idx, client in enumerate(clients):\n",
    "        client.imp_model.load_state_dict(avg_params[idx])\n",
    "        \n",
    "    # evaluation\n",
    "    evaluate_model(clients)\n",
    "\n",
    "for it in range(num_ft_steps):\n",
    "    print(f\"Fine-tuning step {it}\")\n",
    "    \n",
    "    # local model training\n",
    "    params = []\n",
    "    for client in clients:\n",
    "        params.append(\n",
    "            local_train(client.imp_model, client.X_imp, client.mask, train_params, client.X, return_params=True)\n",
    "        )\n",
    "        \n",
    "    # evaluation\n",
    "    evaluate_model(clients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fed_imp2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
