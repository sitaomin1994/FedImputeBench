{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "cd .."
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from src.loaders.load_data import load_data\n",
    "from src.loaders.load_data_partition import load_data_partition, separate_data\n",
    "from src.modules.missing_simulate.mcar_simulate import simulate_nan_mcar\n",
    "from src.modules.missing_simulate.mar_simulate import simulate_nan_mar_sigmoid, simulate_nan_mar_quantile"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "source": [
    "train_data, test_data, config = load_data('codon', test_size=0.1)\n",
    "train_data = pd.DataFrame(train_data).sample(frac=0.5).values\n",
    "print(train_data.shape, test_data.shape)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "source": [
    "X, y, stats = separate_data(\n",
    "    train_data, num_clients = 10, niid = True, alpha = 0.05, least_samples = 50, local_test_ratio=0\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "source": [
    "X_ms = []\n",
    "for X_origin in X:\n",
    "    X_ms.append(simulate_nan_mcar(X_origin, cols = np.arange(X_origin.shape[1]), missing_ratio=0.5))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "source": [
    "X_ms = []\n",
    "seeds = 0 \n",
    "for X_origin in X:\n",
    "    X_ms.append(\n",
    "        simulate_nan_mar_sigmoid(\n",
    "            X_origin, cols = np.arange(X_origin.shape[1] - 3), missing_ratio=0.5, obs = True, strict = True, k = 'all',\n",
    "            missing_func=str(np.random.choice(['left', 'right']))\n",
    "        )\n",
    "    )"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "source": [
    "class Client:\n",
    "    def __init__(self, X, X_ms, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.X_ms = X_ms\n",
    "        self.mask = np.isnan(X_ms)\n",
    "        self.X_imp = X_ms.copy()\n",
    "        self.X_imp[self.mask] = 0\n",
    "        self.imp_model = None"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "source": [
    "# stdlib\n",
    "from typing import Any, List, Dict, Tuple\n",
    "\n",
    "# third party\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.distributions as td\n",
    "\n",
    "# hyperimpute absolute\n",
    "from emf.reproduce_utils import set_seed\n",
    "from src.imputation.models.vae_models.decoder import GaussianDecoder, BernoulliDecoder, StudentTDecoder\n",
    "from src.imputation.models.vae_models.encoder import BaseEncoder\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def weights_init(layer: Any) -> None:\n",
    "    if type(layer) == nn.Linear:\n",
    "        torch.nn.init.orthogonal_(layer.weight)\n",
    "\n",
    "\n",
    "class MIWAE(nn.Module):\n",
    "    \"\"\"MIWAE imputation plugin\n",
    "\n",
    "    Args:\n",
    "        n_epochs: int\n",
    "            Number of training iterations\n",
    "        batch_size: int\n",
    "            Batch size\n",
    "        latent_size: int\n",
    "            dimension of the latent space\n",
    "        n_hidden: int\n",
    "            number of hidden units\n",
    "        K: int\n",
    "            number of IS during training\n",
    "        random_state: int\n",
    "            random seed\n",
    "\n",
    "    Reference: \"MIWAE: Deep Generative Modelling and Imputation of Incomplete Data\", Pierre-Alexandre Mattei,\n",
    "    Jes Frellsen\n",
    "    Original code: https://github.com/pamattei/miwae\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_features: int,\n",
    "            latent_size: int = 1,\n",
    "            n_hidden: int = 16,\n",
    "            n_hidden_layers: int = 2,\n",
    "            seed: int = 0,\n",
    "            out_dist='studentt',\n",
    "            K: int = 20,\n",
    "            L: int = 1000,\n",
    "    ) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "        set_seed(seed)\n",
    "\n",
    "        # parameters\n",
    "        self.num_features = num_features\n",
    "        self.n_hidden = n_hidden  # number of hidden units in (same for all MLPs)\n",
    "        self.n_hidden_layers = n_hidden_layers  # number of hidden layers in (same for all MLPs)\n",
    "        self.latent_size = latent_size  # dimension of the latent space\n",
    "        self.K = K  # number of IS during training\n",
    "        self.L = L  # number of samples for imputation\n",
    "\n",
    "        # encoder\n",
    "        self.encoder = BaseEncoder(\n",
    "            self.num_features, self.latent_size, [self.n_hidden for _ in range(self.n_hidden_layers)],\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        # decoder\n",
    "        self.out_dist = out_dist\n",
    "        if out_dist == 'studentt':\n",
    "            self.decoder = StudentTDecoder(\n",
    "                self.latent_size, self.num_features, [self.n_hidden for _ in range(self.n_hidden_layers)],\n",
    "            )\n",
    "        elif out_dist == 'gaussian':\n",
    "            self.decoder = GaussianDecoder(\n",
    "                self.latent_size, self.num_features, [self.n_hidden for _ in range(self.n_hidden_layers)],\n",
    "            )\n",
    "        elif out_dist == 'bernoulli':\n",
    "            self.decoder = BernoulliDecoder(\n",
    "                self.latent_size, self.num_features, [self.n_hidden for _ in range(self.n_hidden_layers)],\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"Invalid output distribution\")\n",
    "\n",
    "        self.decoder = self.decoder.to(DEVICE)\n",
    "        \n",
    "        # mapping z to mask using hint\n",
    "        self.mask_net = nn.Sequential(\n",
    "            nn.Linear(self.latent_size, self.n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.n_hidden, self.n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.n_hidden, self.num_features),\n",
    "            nn.Sigmoid()\n",
    "        ).to(DEVICE)\n",
    "        \n",
    "        self.z_L = None\n",
    "        self.mask_L = None\n",
    "\n",
    "        # prior for z\n",
    "        self.p_z = td.Independent(\n",
    "            td.Normal(loc=torch.zeros(self.latent_size).to(DEVICE), scale=torch.ones(self.latent_size).to(DEVICE)), 1\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def name() -> str:\n",
    "        return \"miwae\"\n",
    "\n",
    "    def init(self, seed):\n",
    "        set_seed(seed)\n",
    "        self.encoder.apply(weights_init)\n",
    "        self.decoder.apply(weights_init)\n",
    "        self.mask_net.apply(weights_init)\n",
    "\n",
    "    def compute_loss(self, inputs: List[torch.Tensor]) -> Tuple[torch.Tensor, Dict]:\n",
    "        x, mask = inputs  # x - data, mask - missing mask\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        # encoder\n",
    "        mu, logvar = self.encoder(x)\n",
    "\n",
    "        q_zgivenxobs = td.Independent(td.Normal(loc=mu, scale=logvar), 1)\n",
    "        zgivenx = q_zgivenxobs.rsample([self.K])  # shape (K, batch_size, latent_size)\n",
    "        zgivenx_flat = zgivenx.reshape([self.K * batch_size, self.latent_size])\n",
    "        \n",
    "        self.z_L = zgivenx_flat\n",
    "\n",
    "        # decoder\n",
    "        out_decoder = self.decoder(zgivenx_flat)\n",
    "        recon_x_means = self.decoder.l_out_mu(out_decoder)\n",
    "\n",
    "        # compute loss\n",
    "        data_flat = torch.Tensor.repeat(x, [self.K, 1]).reshape([-1, 1]).to(DEVICE)\n",
    "        tiled_mask = torch.Tensor.repeat(mask, [self.K, 1]).to(DEVICE)\n",
    "        self.mask_L = tiled_mask\n",
    "\n",
    "        # p(x|z)\n",
    "        all_log_pxgivenz_flat = self.decoder.dist_xgivenz(out_decoder, flat=True).log_prob(data_flat)\n",
    "        all_log_pxgivenz = all_log_pxgivenz_flat.reshape([self.K * batch_size, self.num_features])\n",
    "        logpxobsgivenz = torch.sum(all_log_pxgivenz * tiled_mask, 1).reshape([self.K, batch_size])\n",
    "\n",
    "        # p(z) and q(z|x)\n",
    "        logpz = self.p_z.log_prob(zgivenx)\n",
    "        logq = q_zgivenxobs.log_prob(zgivenx)\n",
    "\n",
    "        neg_bound = -torch.mean(torch.logsumexp(logpxobsgivenz + logpz - logq, 0))\n",
    "\n",
    "        return neg_bound, {}\n",
    "    \n",
    "    def mask_prediction_loss(self, x, mask):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            batch_size = x.shape[0]\n",
    "            mu, logvar = self.encoder(x)\n",
    "            q_zgivenxobs = td.Independent(td.Normal(loc=mu, scale=logvar), 1)\n",
    "            zgivenx = q_zgivenxobs.rsample([self.K])  # shape (K, batch_size, latent_size)\n",
    "            zgivenx_flat = zgivenx.reshape([self.K * batch_size, self.latent_size])\n",
    "            z = zgivenx_flat\n",
    "            tiled_mask = torch.Tensor.repeat(mask, [self.K, 1]).to(DEVICE)\n",
    "            mask = tiled_mask.reshape([-1, self.num_features]).int()\n",
    "            mask = mask.float()\n",
    "            #z = torch.cat([z, mask], 1)\n",
    "        \n",
    "        mask_pred = self.mask_net(z)\n",
    "        loss = torch.nn.BCELoss()(mask_pred, mask)  # predict missing prob -> 1\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def mask_prediction(self, x: torch.Tensor, mask: torch.Tensor, K: int = 10):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            mu, logvar = self.encoder(x)\n",
    "            q_zgivenxobs = td.Independent(td.Normal(loc=mu, scale=logvar), 1)\n",
    "            zgivenx = q_zgivenxobs.rsample([K])  # shape (K, batch_size, latent_size)\n",
    "            zgivenx_flat = zgivenx.reshape([K * x.shape[0], self.latent_size])\n",
    "            mask_L = torch.Tensor.repeat(mask, [K, 1])\n",
    "            #zgivenx_flat = torch.cat([zgivenx_flat, mask_L.reshape([-1, self.num_features]).float()], 1)\n",
    "            \n",
    "            mask_pred = self.mask_net(zgivenx_flat)\n",
    "            mask_pred = mask_pred.reshape([K, x.shape[0], self.num_features])\n",
    "            \n",
    "            return mask_pred\n",
    "\n",
    "    def impute(self, x: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        L = self.L\n",
    "        batch_size = x.shape[0]\n",
    "        p = x.shape[1]\n",
    "\n",
    "        # encoder\n",
    "        self.encoder.to(DEVICE)\n",
    "        self.decoder.to(DEVICE)\n",
    "        mu, logvar = self.encoder(x)\n",
    "        q_zgivenxobs = td.Independent(td.Normal(loc=mu, scale=logvar), 1)\n",
    "\n",
    "        zgivenx = q_zgivenxobs.rsample([L])\n",
    "        zgivenx_flat = zgivenx.reshape([L * batch_size, self.latent_size])\n",
    "\n",
    "        # decoder\n",
    "        out_decoder = self.decoder(zgivenx_flat)\n",
    "        recon_x_means = self.decoder.l_out_mu(out_decoder)\n",
    "\n",
    "        # loss\n",
    "        data_flat = torch.Tensor.repeat(x, [L, 1]).reshape([-1, 1]).to(DEVICE)\n",
    "        tiledmask = torch.Tensor.repeat(mask, [L, 1]).to(DEVICE)\n",
    "\n",
    "        all_log_pxgivenz_flat = self.decoder.dist_xgivenz(out_decoder, flat=True).log_prob(data_flat)\n",
    "        all_log_pxgivenz = all_log_pxgivenz_flat.reshape([L * batch_size, p])\n",
    "\n",
    "        logpxobsgivenz = torch.sum(all_log_pxgivenz * tiledmask, 1).reshape([L, batch_size])\n",
    "        logpz = self.p_z.log_prob(zgivenx)\n",
    "        logq = q_zgivenxobs.log_prob(zgivenx)\n",
    "\n",
    "        # imputation weighted samples\n",
    "        imp_weights = torch.nn.functional.softmax(\n",
    "            logpxobsgivenz + logpz - logq, 0\n",
    "        )  # these are w_1,....,w_L for all observations in the batch\n",
    "\n",
    "        xgivenz = self.decoder.imp_dist_xgivenz(out_decoder)\n",
    "        xms = xgivenz.sample().reshape([L, batch_size, p])\n",
    "        xm = torch.einsum(\"ki,kij->ij\", imp_weights, xms)\n",
    "\n",
    "        # merge imputed values with observed values\n",
    "        xhat = torch.clone(x)\n",
    "        xhat[~mask.bool()] = xm[~mask.bool()]\n",
    "\n",
    "        return xhat\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "source": [
    "\n",
    "from copy import deepcopy\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "from src.evaluation.imp_quality_metrics import rmse\n",
    "\n",
    "def local_train(model, X, mask, train_params, X_true, return_params = False):\n",
    "    \n",
    "    model.to(DEVICE)\n",
    "    lr = train_params['lr']\n",
    "    weight_decay = train_params['weight_decay']\n",
    "    epochs = train_params['epochs']\n",
    "    batch_size = train_params['batch_size']\n",
    "    verbose = train_params['verbose']\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay, amsgrad=True)\n",
    "\n",
    "    # data\n",
    "    n = X.shape[0]\n",
    "    X_imp = X.copy()\n",
    "    X_mask = mask.copy()\n",
    "    bs = min(batch_size, n)\n",
    "\n",
    "    final_loss = 0\n",
    "    rmses = []\n",
    "    for ep in range(epochs):\n",
    "        \n",
    "        # evaluation\n",
    "        with torch.no_grad():\n",
    "            X_imp_new = model.impute(\n",
    "                torch.from_numpy(X_imp).float().to(DEVICE), torch.from_numpy(~X_mask).float().to(DEVICE)\n",
    "            )\n",
    "            rmse_value = rmse(X_imp_new.detach().clone().cpu().numpy(), X_true, X_mask)\n",
    "            rmses.append(rmse_value)\n",
    "\n",
    "        # shuffle data\n",
    "        perm = np.random.permutation(n)  # We use the \"random reshuffling\" version of SGD\n",
    "        batches_data = np.array_split(X_imp[perm,], int(n / bs), )\n",
    "        batches_mask = np.array_split(X_mask[perm,], int(n / bs), )\n",
    "        total_loss, total_iters = 0, 0\n",
    "        total_mask_loss = 0\n",
    "        model.train()\n",
    "        for it in range(len(batches_data)):\n",
    "            optimizer.zero_grad()\n",
    "            model.encoder.zero_grad()\n",
    "            model.decoder.zero_grad()\n",
    "            model.mask_net.zero_grad()\n",
    "            b_data = torch.from_numpy(batches_data[it]).float().to(DEVICE)\n",
    "            b_mask = torch.from_numpy(~batches_mask[it]).float().to(DEVICE)\n",
    "            data = [b_data, b_mask]\n",
    "            loss, ret_dict = model.compute_loss(data)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_iters += 1\n",
    "\n",
    "\n",
    "        # print loss\n",
    "        if (ep + 1) % verbose == 0:\n",
    "            print('Epoch %s/%s, Loss = %s RMSE = %s' % (ep, epochs, total_loss / total_iters, rmses[-1]))\n",
    "\n",
    "        if DEVICE == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        final_loss = total_loss / total_iters\n",
    "\n",
    "    model.to(\"cpu\")\n",
    "    \n",
    "    if return_params:\n",
    "        return deepcopy(model.state_dict())\n",
    "    else:\n",
    "        return model\n",
    "\n",
    "def impute(model, X, mask):\n",
    "    model.to(DEVICE)\n",
    "    X_imp = X.copy()\n",
    "    X_mask = mask.copy()\n",
    "    X_imp[mask] = 0\n",
    "    X_imp = torch.from_numpy(X_imp).float().to(DEVICE)\n",
    "    X_mask = torch.from_numpy(~X_mask).float().to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        X_imp_new = model.impute(X_imp, X_mask)\n",
    "    model.to(\"cpu\")\n",
    "    return X_imp_new.detach().clone().cpu().numpy()\n",
    "\n",
    "\n",
    "def evaluate_model(clients):\n",
    "    rmses = []\n",
    "    for client in clients:\n",
    "        client.X_imp = impute(client.imp_model, client.X_ms, client.mask)\n",
    "        rmses.append(rmse(client.X_imp, client.X, client.mask))\n",
    "    print(\"RMSE Mean: {:.4f} Std: {:.4f} Max: {:.4f} Min: {:.4f} \".format(\n",
    "        np.mean(rmses), np.std(rmses), np.max(rmses), np.min(rmses)\n",
    "    ))\n",
    "            "
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "source": [
    "clients = [Client(X[i].copy(), X_ms[i].copy(), y[i].copy()) for i in range(len(X))]\n",
    "for idx, client in enumerate(clients):\n",
    "    client.imp_model = MIWAE(\n",
    "        client.X.shape[1], latent_size=10, n_hidden=32, n_hidden_layers=2, seed=idx, out_dist='gaussian', K=20, L=100, \n",
    "    )\n",
    "    \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.heatmap(clients[0].mask)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "source": [
    "global_iteration = 100\n",
    "\n",
    "train_params = {\n",
    "    'lr': 1e-3,\n",
    "    'weight_decay': 1e-5,\n",
    "    'epochs': 5,\n",
    "    'batch_size': 64,\n",
    "    'verbose': 10\n",
    "}\n",
    "\n",
    "for it in range(global_iteration):\n",
    "    print(f\"Global iteration {it}\")\n",
    "    \n",
    "    # local model training\n",
    "    params = []\n",
    "    for client in clients:\n",
    "        params.append(\n",
    "            local_train(client.imp_model, client.X_imp, client.mask, train_params, client.X, return_params=True)\n",
    "        )\n",
    "    \n",
    "    fit_res = [{'sample_size': client.X.shape[0]} for client in clients]\n",
    "    \n",
    "    # # federated averaging\n",
    "    # avg_params, _ = fedavg(params, fit_res)\n",
    "    \n",
    "    # for idx, client in enumerate(clients):\n",
    "    #     client.imp_model.load_state_dict(avg_params[idx])\n",
    "        \n",
    "    # evaluation\n",
    "    evaluate_model(clients)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FedAvg Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "source": [
    "def fedavg(local_model_parameters, fit_res):\n",
    "    # federated averaging implementation\n",
    "    averaged_model_state_dict = OrderedDict()  # global parameters\n",
    "    sample_sizes = np.array([item['sample_size'] for item in fit_res])\n",
    "    normed_weights = sample_sizes / np.sum(sample_sizes)\n",
    "\n",
    "    for it, local_model_state_dict in enumerate(local_model_parameters):\n",
    "        for key in local_model_state_dict.keys():\n",
    "            if it == 0:\n",
    "                averaged_model_state_dict[key] = normed_weights[it]*local_model_state_dict[key]\n",
    "            else:\n",
    "                averaged_model_state_dict[key] += normed_weights[it]*local_model_state_dict[key]\n",
    "\n",
    "    # copy parameters for each client\n",
    "    agg_model_parameters = [deepcopy(averaged_model_state_dict) for _ in range(len(local_model_parameters))]\n",
    "    agg_res = {}\n",
    "\n",
    "    return agg_model_parameters, agg_res"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "source": [
    "clients = [Client(X[i].copy(), X_ms[i].copy(), y[i].copy()) for i in range(len(X))]\n",
    "for idx, client in enumerate(clients):\n",
    "    client.imp_model = MIWAE(\n",
    "        client.X.shape[1], latent_size=10, n_hidden=32, n_hidden_layers=2, seed=idx, out_dist='gaussian', K=20, L=100\n",
    "    )"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "source": [
    "global_iteration = 50\n",
    "\n",
    "train_params = {\n",
    "    'lr': 1e-3,\n",
    "    'weight_decay': 1e-5,\n",
    "    'epochs': 5,\n",
    "    'batch_size': 64,\n",
    "    'verbose': 10\n",
    "}\n",
    "\n",
    "for it in range(global_iteration):\n",
    "    print(f\"Global iteration {it}\")\n",
    "    \n",
    "    # local model training\n",
    "    params = []\n",
    "    for client in clients:\n",
    "        params.append(\n",
    "            local_train(client.imp_model, client.X_imp, client.mask, train_params, client.X, return_params=True)\n",
    "        )\n",
    "    \n",
    "    fit_res = [{'sample_size': client.X.shape[0]} for client in clients]\n",
    "    \n",
    "    # federated averaging\n",
    "    avg_params, _ = fedavg(params, fit_res)\n",
    "    \n",
    "    for idx, client in enumerate(clients):\n",
    "        client.imp_model.load_state_dict(avg_params[idx])\n",
    "        \n",
    "    # evaluation\n",
    "    evaluate_model(clients)\n",
    "    "
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FedAvg FT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "source": [
    "clients = [Client(X[i].copy(), X_ms[i].copy(), y[i].copy()) for i in range(len(X))]\n",
    "for idx, client in enumerate(clients):\n",
    "    client.imp_model = MIWAE(\n",
    "        client.X.shape[1], latent_size=10, n_hidden=32, n_hidden_layers=2, seed=idx, out_dist='gaussian', K=20, L=100\n",
    "    )"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "source": [
    "global_iteration = 60\n",
    "num_ft_steps = 40\n",
    "\n",
    "train_params = {\n",
    "    'lr': 1e-3,\n",
    "    'weight_decay': 1e-5,\n",
    "    'epochs': 5,\n",
    "    'batch_size': 64,\n",
    "    'verbose': 10\n",
    "}\n",
    "\n",
    "for it in range(global_iteration):\n",
    "    print(f\"Global iteration {it}\")\n",
    "    \n",
    "    # local model training\n",
    "    params = []\n",
    "    for client in clients:\n",
    "        params.append(\n",
    "            local_train(client.imp_model, client.X_imp, client.mask, train_params, client.X, return_params=True)\n",
    "        )\n",
    "    \n",
    "    fit_res = [{'sample_size': client.X.shape[0]} for client in clients]\n",
    "    \n",
    "    # federated averaging\n",
    "    avg_params, _ = fedavg(params, fit_res)\n",
    "    \n",
    "    for idx, client in enumerate(clients):\n",
    "        client.imp_model.load_state_dict(avg_params[idx])\n",
    "        \n",
    "    # evaluation\n",
    "    evaluate_model(clients)\n",
    "\n",
    "for it in range(num_ft_steps):\n",
    "    print(f\"Fine-tuning step {it}\")\n",
    "    \n",
    "    # local model training\n",
    "    params = []\n",
    "    for client in clients:\n",
    "        params.append(\n",
    "            local_train(client.imp_model, client.X_imp, client.mask, train_params, client.X, return_params=True)\n",
    "        )\n",
    "        \n",
    "    # evaluation\n",
    "    evaluate_model(clients)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FedProx and FT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "source": [
    "from torch.optim import Optimizer\n",
    "import copy\n",
    "\n",
    "class FedProxOptimizer(Optimizer):\n",
    "    def __init__(self, params, lr=0.01, lamda=0.1, mu=0.001):\n",
    "        if lr < 0.0:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        defaults=dict(lr=lr, lamda=lamda, mu=mu)\n",
    "        super(FedProxOptimizer, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self, vstar, closure=None):\n",
    "        loss=None\n",
    "        if closure is not None:\n",
    "            loss=closure\n",
    "        for group in self.param_groups:\n",
    "            for p, pstar in zip(group['params'], vstar):\n",
    "                # w <=== w - lr * ( w'  + lambda * (w - w* ) + mu * w )\n",
    "                p.data=p.data - group['lr'] * (\n",
    "                            p.grad.data + group['lamda'] * (p.data - pstar.data.clone()) + group['mu'] * p.data)\n",
    "        return group['params'], loss\n",
    "\n",
    "def local_train_prox(model, X, mask, train_params, X_true, return_params = False, mu = 0.001):\n",
    "    \n",
    "    global_model = MIWAE(\n",
    "        X.shape[1], latent_size=10, n_hidden=32, n_hidden_layers=2, seed=0, out_dist='gaussian', K=20, L=100\n",
    "    )\n",
    "    global_model.load_state_dict(copy.deepcopy(model.state_dict()))\n",
    "    model.to(DEVICE)\n",
    "    global_model.to(DEVICE)\n",
    "    lr = train_params['lr']\n",
    "    weight_decay = train_params['weight_decay']\n",
    "    epochs = train_params['epochs']\n",
    "    batch_size = train_params['batch_size']\n",
    "    verbose = train_params['verbose']\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay, amsgrad=True)\n",
    "\n",
    "    # data\n",
    "    n = X.shape[0]\n",
    "    X_imp = X.copy()\n",
    "    X_mask = mask.copy()\n",
    "    bs = min(batch_size, n)\n",
    "\n",
    "    final_loss = 0\n",
    "    rmses = []\n",
    "    for ep in range(epochs):\n",
    "        \n",
    "        # evaluation\n",
    "        with torch.no_grad():\n",
    "            X_imp_new = model.impute(\n",
    "                torch.from_numpy(X_imp).float().to(DEVICE), torch.from_numpy(~X_mask).float().to(DEVICE)\n",
    "            )\n",
    "            rmse_value = rmse(X_imp_new.detach().clone().cpu().numpy(), X_true, X_mask)\n",
    "            rmses.append(rmse_value)\n",
    "\n",
    "        # shuffle data\n",
    "        perm = np.random.permutation(n)  # We use the \"random reshuffling\" version of SGD\n",
    "        batches_data = np.array_split(X_imp[perm,], int(n / bs), )\n",
    "        batches_mask = np.array_split(X_mask[perm,], int(n / bs), )\n",
    "        total_loss, total_iters = 0, 0\n",
    "        total_mask_loss = 0\n",
    "        model.train()\n",
    "        for it in range(len(batches_data)):\n",
    "            optimizer.zero_grad()\n",
    "            model.encoder.zero_grad()\n",
    "            model.decoder.zero_grad()\n",
    "            model.mask_net.zero_grad()\n",
    "            b_data = torch.from_numpy(batches_data[it]).float().to(DEVICE)\n",
    "            b_mask = torch.from_numpy(~batches_mask[it]).float().to(DEVICE)\n",
    "            data = [b_data, b_mask]\n",
    "            loss, ret_dict = model.compute_loss(data)\n",
    "            proximal_term = 0.0\n",
    "            for w, w_t in zip(model.parameters(), global_model.parameters()):\n",
    "                proximal_term += (w - w_t).norm(2)\n",
    "            loss = loss + (mu / 2) * proximal_term\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_iters += 1\n",
    "\n",
    "\n",
    "        # print loss\n",
    "        if (ep + 1) % verbose == 0:\n",
    "            print('Epoch %s/%s, Loss = %s RMSE = %s' % (ep, epochs, total_loss / total_iters, rmses[-1]))\n",
    "\n",
    "        if DEVICE == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        final_loss = total_loss / total_iters\n",
    "\n",
    "    model.to(\"cpu\")\n",
    "    global_model.to(\"cpu\")\n",
    "    del global_model\n",
    "    \n",
    "    if return_params:\n",
    "        return deepcopy(model.state_dict())\n",
    "    else:\n",
    "        return model"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "source": [
    "clients = [Client(X[i].copy(), X_ms[i].copy(), y[i].copy()) for i in range(len(X))]\n",
    "for idx, client in enumerate(clients):\n",
    "    client.imp_model = MIWAE(\n",
    "        client.X.shape[1], latent_size=10, n_hidden=32, n_hidden_layers=2, seed=idx, out_dist='gaussian', K=20, L=100\n",
    "    )"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "source": [
    "global_iteration = 50\n",
    "num_ft_steps = 50\n",
    "\n",
    "train_params = {\n",
    "    'lr': 1e-3,\n",
    "    'weight_decay': 1e-5,\n",
    "    'epochs': 5,\n",
    "    'batch_size': 64,\n",
    "    'verbose': 10\n",
    "}\n",
    "\n",
    "for it in range(global_iteration):\n",
    "    print(f\"Global iteration {it}\")\n",
    "    \n",
    "    # local model training\n",
    "    params = []\n",
    "    for client in clients:\n",
    "        params.append(\n",
    "            local_train_prox(client.imp_model, client.X_imp, client.mask, train_params, client.X, return_params=True)\n",
    "        )\n",
    "    \n",
    "    fit_res = [{'sample_size': client.X.shape[0]} for client in clients]\n",
    "    \n",
    "    # federated averaging\n",
    "    avg_params, _ = fedavg(params, fit_res)\n",
    "    \n",
    "    for idx, client in enumerate(clients):\n",
    "        client.imp_model.load_state_dict(avg_params[idx])\n",
    "        \n",
    "    # evaluation\n",
    "    evaluate_model(clients)\n",
    "\n",
    "for it in range(num_ft_steps):\n",
    "    print(f\"Fine-tuning step {it}\")\n",
    "    \n",
    "    # local model training\n",
    "    params = []\n",
    "    for client in clients:\n",
    "        params.append(\n",
    "            local_train(client.imp_model, client.X_imp, client.mask, train_params, client.X, return_params=True)\n",
    "        )\n",
    "        \n",
    "    # evaluation\n",
    "    evaluate_model(clients)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fed_imp2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
