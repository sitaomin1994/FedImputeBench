{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T02:29:21.929313Z",
     "start_time": "2024-04-03T02:29:21.924935Z"
    }
   },
   "source": [
    "cd .."
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T02:29:23.547356Z",
     "start_time": "2024-04-03T02:29:22.912076Z"
    }
   },
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "%load_ext dotenv\n",
    "%dotenv\n",
    "import pandas as pd\n",
    "from src.loaders.load_data import load_data"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "train, test, data_config = load_data('codrna')\n",
    "train_df = pd.DataFrame(train).sample(frac=0.05, random_state=42)\n",
    "X = train_df.iloc[:, :-1].values\n",
    "print(X.shape)\n",
    "train_df"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "housing = load_breast_cancer()\n",
    "frac = 1\n",
    "X = pd.DataFrame(housing.data)\n",
    "y = pd.DataFrame(housing.target)\n",
    "df = pd.concat([X, y], axis=1)\n",
    "df = df.sample(frac=frac, random_state=42)\n",
    "X = df.iloc[:, :-1].values\n",
    "X = (X - X.min(axis = 1, keepdims=True)) / (X.max(axis = 1, keepdims=True) - X.min(axis = 1, keepdims=True) + 0.001)\n",
    "y = df.iloc[:, -1].values\n",
    "print(X.shape)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "source": [
    "df"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "source": [
    "datas = []\n",
    "for i in range(10):\n",
    "    datas.append(df.iloc[i, :].values)\n",
    "\n",
    "for item in zip(*datas):\n",
    "    string = \"{:10.2f} \"*len(item)\n",
    "    print(string.format(*item))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "embedding = TruncatedSVD(n_components=2)\n",
    "X_transformed = embedding.fit_transform(X)\n",
    "print(X_transformed.shape)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "source": [
    "from sklearn.manifold import TSNE\n",
    "embedding = TSNE(n_components=2, perplexity=50, random_state=42)\n",
    "X_transformed = embedding.fit_transform(X)\n",
    "print(X_transformed.shape)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "source": [
    "from sklearn.manifold import SpectralEmbedding\n",
    "embedding = SpectralEmbedding(n_components=2, random_state=42)\n",
    "X_transformed = embedding.fit_transform(X)\n",
    "print(X_transformed.shape)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.scatterplot(x=X_transformed[:, 0], y=X_transformed[:, 1], hue = y, palette='viridis')\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "corr = pd.DataFrame(X).corr().abs()\n",
    "mask = np.zeros_like(corr, dtype=bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "sns.heatmap(corr, annot=False, cmap='viridis')\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "# embedding = TruncatedSVD(n_components=2)\n",
    "# X_transformed = embedding.fit_transform(X)\n",
    "# print(X_transformed.shape)\n",
    "\n",
    "pca = PCA(n_components=5, random_state=42)\n",
    "X_transformed = pca.fit_transform(X)\n",
    "print(X_transformed.shape)\n",
    "\n",
    "corr_with_component = np.zeros((X_transformed.shape[1], X.shape[1]))\n",
    "for components_id in range(X_transformed.shape[1]):\n",
    "    corr_with_component[components_id, : ] = pd.DataFrame(X).corrwith(pd.Series(X_transformed[:, components_id])).values\n",
    "fig, ax = plt.subplots()   \n",
    "sns.heatmap(corr_with_component, annot=False, ax=ax)\n",
    "\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "source": [
    "na_inds = np.where(X_transformed[:, 4] > X_transformed[:, 4].mean())\n",
    "na_inds"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## MIWAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T14:19:13.782852Z",
     "start_time": "2024-04-03T14:19:13.774523Z"
    },
    "collapsed": false
   },
   "source": [
    "from sklearn.datasets import make_gaussian_quantiles, make_hastie_10_2, make_blobs, make_classification\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "X,y = make_classification(n_samples = 1000,  n_features=6, n_informative=2, n_redundant=1, random_state = 42)\n",
    "X = StandardScaler().fit_transform(X)\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "print(X.shape, y.shape)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T14:19:14.644872Z",
     "start_time": "2024-04-03T14:19:14.489078Z"
    },
    "collapsed": false
   },
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_transformed = pca.fit_transform(X)\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.scatterplot(x=X_transformed[:, 0], y=X_transformed[:, 1], hue = y, palette='viridis')\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T14:19:15.568882Z",
     "start_time": "2024-04-03T14:19:15.411708Z"
    },
    "collapsed": false
   },
   "source": [
    "sns.heatmap(pd.DataFrame(X).corr())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T19:14:25.756599Z",
     "start_time": "2024-04-03T19:14:25.541581Z"
    },
    "collapsed": false
   },
   "source": [
    "# generate missing data\n",
    "import numpy as np\n",
    "\n",
    "missing_ratio = 0.5\n",
    "X1 = X.copy()\n",
    "X_missing1 = X.copy()\n",
    "np.random.seed(42)\n",
    "X_missing1[X_missing1[:, 0] < np.quantile(X_missing1[:, 0], 0.5), 0] = np.nan\n",
    "X_missing1[X_missing1[:, 1] > np.quantile(X_missing1[:, 1], 0.5), 1] = np.nan\n",
    "X_missing1[X_missing1[:, 2] < np.quantile(X_missing1[:, 2], 0.5), 2] = np.nan\n",
    "X_missing1[X_missing1[:, 3] > np.quantile(X_missing1[:, 3], 0.5), 3] = np.nan\n",
    "X_missing1[X_missing1[:, 4] < np.quantile(X_missing1[:, 4], 0.5), 4] = np.nan\n",
    "X_missing1[X_missing1[:, 5] > np.quantile(X_missing1[:, 5], 0.5), 5] = np.nan\n",
    "mask1 = np.isnan(X_missing1)\n",
    "\n",
    "# missing pattern\n",
    "pattern = pd.DataFrame(mask1).astype(int).astype(str).apply(lambda x: ''.join(x), axis=1)\n",
    "print(pattern.value_counts())\n",
    "\n",
    "sorted_mask1 = pd.DataFrame(mask1).reindex(pattern.sort_values().index).values\n",
    "mask1 = sorted_mask1\n",
    "X1 = pd.DataFrame(X1).reindex(pattern.sort_values().index).values\n",
    "X_missing1 = pd.DataFrame(X_missing1).reindex(pattern.sort_values().index).values\n",
    "X_imp1 = X_missing1.copy()\n",
    "X_imp1[mask1] = 0\n",
    "sns.heatmap(mask1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T19:15:04.660570Z",
     "start_time": "2024-04-03T19:15:04.444019Z"
    },
    "collapsed": false
   },
   "source": [
    "# generate missing data\n",
    "import numpy as np\n",
    "\n",
    "X2 = X.copy()\n",
    "X_missing2 = X.copy()\n",
    "np.random.seed(42)\n",
    "X_missing2[X_missing2[:, 0] > np.quantile(X_missing2[:, 0], 0.5), 0] = np.nan\n",
    "X_missing2[X_missing2[:, 1] < np.quantile(X_missing2[:, 1], 0.5), 1] = np.nan\n",
    "X_missing2[X_missing2[:, 2] > np.quantile(X_missing2[:, 2], 0.5), 2] = np.nan\n",
    "X_missing2[X_missing2[:, 3] < np.quantile(X_missing2[:, 3], 0.5), 3] = np.nan\n",
    "X_missing2[X_missing2[:, 4] > np.quantile(X_missing2[:, 4], 0.5), 4] = np.nan\n",
    "X_missing2[X_missing2[:, 5] < np.quantile(X_missing2[:, 5], 0.5), 5] = np.nan\n",
    "mask2 = np.isnan(X_missing2)\n",
    "\n",
    "# missing pattern\n",
    "pattern = pd.DataFrame(mask2).astype(int).astype(str).apply(lambda x: ''.join(x), axis=1)\n",
    "print(pattern.value_counts())\n",
    "\n",
    "sorted_mask2 = pd.DataFrame(mask2).reindex(pattern.sort_values().index).values\n",
    "mask2 = sorted_mask2\n",
    "X2 = pd.DataFrame(X2).reindex(pattern.sort_values().index).values\n",
    "X_missing2 = pd.DataFrame(X_missing2).reindex(pattern.sort_values().index).values\n",
    "X_imp2 = X_missing2.copy()\n",
    "X_imp2[mask2] = 0\n",
    "sns.heatmap(mask2)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T19:15:08.512694Z",
     "start_time": "2024-04-03T19:15:08.495188Z"
    },
    "collapsed": false
   },
   "source": [
    "# stdlib\n",
    "from typing import Any, List, Dict, Tuple\n",
    "\n",
    "# third party\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.distributions as td\n",
    "\n",
    "# hyperimpute absolute\n",
    "from emf.reproduce_utils import set_seed\n",
    "from src.imputation.models.vae_models.decoder import GaussianDecoder, BernoulliDecoder, StudentTDecoder\n",
    "from src.imputation.models.vae_models.encoder import BaseEncoder\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def weights_init(layer: Any) -> None:\n",
    "    if type(layer) == nn.Linear:\n",
    "        torch.nn.init.orthogonal_(layer.weight)\n",
    "\n",
    "\n",
    "class MIWAE(nn.Module):\n",
    "    \"\"\"MIWAE imputation plugin\n",
    "\n",
    "    Args:\n",
    "        n_epochs: int\n",
    "            Number of training iterations\n",
    "        batch_size: int\n",
    "            Batch size\n",
    "        latent_size: int\n",
    "            dimension of the latent space\n",
    "        n_hidden: int\n",
    "            number of hidden units\n",
    "        K: int\n",
    "            number of IS during training\n",
    "        random_state: int\n",
    "            random seed\n",
    "\n",
    "    Reference: \"MIWAE: Deep Generative Modelling and Imputation of Incomplete Data\", Pierre-Alexandre Mattei,\n",
    "    Jes Frellsen\n",
    "    Original code: https://github.com/pamattei/miwae\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_features: int,\n",
    "            latent_size: int = 1,\n",
    "            n_hidden: int = 16,\n",
    "            n_hidden_layers: int = 2,\n",
    "            seed: int = 0,\n",
    "            out_dist='studentt',\n",
    "            K: int = 20,\n",
    "            L: int = 1000,\n",
    "    ) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "        set_seed(seed)\n",
    "\n",
    "        # parameters\n",
    "        self.num_features = num_features\n",
    "        self.n_hidden = n_hidden  # number of hidden units in (same for all MLPs)\n",
    "        self.n_hidden_layers = n_hidden_layers  # number of hidden layers in (same for all MLPs)\n",
    "        self.latent_size = latent_size  # dimension of the latent space\n",
    "        self.K = K  # number of IS during training\n",
    "        self.L = L  # number of samples for imputation\n",
    "\n",
    "        # encoder\n",
    "        self.encoder = BaseEncoder(\n",
    "            self.num_features, self.latent_size, [self.n_hidden for _ in range(self.n_hidden_layers)],\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        # decoder\n",
    "        self.out_dist = out_dist\n",
    "        if out_dist == 'studentt':\n",
    "            self.decoder = StudentTDecoder(\n",
    "                self.latent_size, self.num_features, [self.n_hidden for _ in range(self.n_hidden_layers)],\n",
    "            )\n",
    "        elif out_dist == 'gaussian':\n",
    "            self.decoder = GaussianDecoder(\n",
    "                self.latent_size, self.num_features, [self.n_hidden for _ in range(self.n_hidden_layers)],\n",
    "            )\n",
    "        elif out_dist == 'bernoulli':\n",
    "            self.decoder = BernoulliDecoder(\n",
    "                self.latent_size, self.num_features, [self.n_hidden for _ in range(self.n_hidden_layers)],\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"Invalid output distribution\")\n",
    "\n",
    "        self.decoder = self.decoder.to(DEVICE)\n",
    "        \n",
    "        # mapping z to mask using hint\n",
    "        self.mask_net = nn.Sequential(\n",
    "            nn.Linear(self.latent_size, self.n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.n_hidden, self.n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.n_hidden, self.num_features),\n",
    "            nn.Sigmoid()\n",
    "        ).to(DEVICE)\n",
    "        \n",
    "        self.z_L = None\n",
    "        self.mask_L = None\n",
    "\n",
    "        # prior for z\n",
    "        self.p_z = td.Independent(\n",
    "            td.Normal(loc=torch.zeros(self.latent_size).to(DEVICE), scale=torch.ones(self.latent_size).to(DEVICE)), 1\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def name() -> str:\n",
    "        return \"miwae\"\n",
    "\n",
    "    def init(self, seed):\n",
    "        set_seed(seed)\n",
    "        self.encoder.apply(weights_init)\n",
    "        self.decoder.apply(weights_init)\n",
    "        self.mask_net.apply(weights_init)\n",
    "\n",
    "    def compute_loss(self, inputs: List[torch.Tensor]) -> Tuple[torch.Tensor, Dict]:\n",
    "        x, mask = inputs  # x - data, mask - missing mask\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        # encoder\n",
    "        mu, logvar = self.encoder(x)\n",
    "\n",
    "        q_zgivenxobs = td.Independent(td.Normal(loc=mu, scale=logvar), 1)\n",
    "        zgivenx = q_zgivenxobs.rsample([self.K])  # shape (K, batch_size, latent_size)\n",
    "        zgivenx_flat = zgivenx.reshape([self.K * batch_size, self.latent_size])\n",
    "        \n",
    "        self.z_L = zgivenx_flat\n",
    "\n",
    "        # decoder\n",
    "        out_decoder = self.decoder(zgivenx_flat)\n",
    "        recon_x_means = self.decoder.l_out_mu(out_decoder)\n",
    "\n",
    "        # compute loss\n",
    "        data_flat = torch.Tensor.repeat(x, [self.K, 1]).reshape([-1, 1]).to(DEVICE)\n",
    "        tiled_mask = torch.Tensor.repeat(mask, [self.K, 1]).to(DEVICE)\n",
    "        self.mask_L = tiled_mask\n",
    "\n",
    "        # p(x|z)\n",
    "        all_log_pxgivenz_flat = self.decoder.dist_xgivenz(out_decoder, flat=True).log_prob(data_flat)\n",
    "        all_log_pxgivenz = all_log_pxgivenz_flat.reshape([self.K * batch_size, self.num_features])\n",
    "        logpxobsgivenz = torch.sum(all_log_pxgivenz * tiled_mask, 1).reshape([self.K, batch_size])\n",
    "\n",
    "        # p(z) and q(z|x)\n",
    "        logpz = self.p_z.log_prob(zgivenx)\n",
    "        logq = q_zgivenxobs.log_prob(zgivenx)\n",
    "\n",
    "        neg_bound = -torch.mean(torch.logsumexp(logpxobsgivenz + logpz - logq, 0))\n",
    "\n",
    "        return neg_bound, {}\n",
    "    \n",
    "    def mask_prediction_loss(self, x, mask):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            batch_size = x.shape[0]\n",
    "            mu, logvar = self.encoder(x)\n",
    "            q_zgivenxobs = td.Independent(td.Normal(loc=mu, scale=logvar), 1)\n",
    "            zgivenx = q_zgivenxobs.rsample([self.K])  # shape (K, batch_size, latent_size)\n",
    "            zgivenx_flat = zgivenx.reshape([self.K * batch_size, self.latent_size])\n",
    "            z = zgivenx_flat\n",
    "            tiled_mask = torch.Tensor.repeat(mask, [self.K, 1]).to(DEVICE)\n",
    "            mask = tiled_mask.reshape([-1, self.num_features]).int()\n",
    "            mask = mask.float()\n",
    "            #z = torch.cat([z, mask], 1)\n",
    "        \n",
    "        mask_pred = self.mask_net(z)\n",
    "        loss = torch.nn.BCELoss()(mask_pred, mask)  # predict missing prob -> 1\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def mask_prediction(self, x: torch.Tensor, mask: torch.Tensor, K: int = 10):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            mu, logvar = self.encoder(x)\n",
    "            q_zgivenxobs = td.Independent(td.Normal(loc=mu, scale=logvar), 1)\n",
    "            zgivenx = q_zgivenxobs.rsample([K])  # shape (K, batch_size, latent_size)\n",
    "            zgivenx_flat = zgivenx.reshape([K * x.shape[0], self.latent_size])\n",
    "            mask_L = torch.Tensor.repeat(mask, [K, 1])\n",
    "            #zgivenx_flat = torch.cat([zgivenx_flat, mask_L.reshape([-1, self.num_features]).float()], 1)\n",
    "            \n",
    "            mask_pred = self.mask_net(zgivenx_flat)\n",
    "            mask_pred = mask_pred.reshape([K, x.shape[0], self.num_features])\n",
    "            \n",
    "            return mask_pred\n",
    "\n",
    "    def impute(self, x: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        L = self.L\n",
    "        batch_size = x.shape[0]\n",
    "        p = x.shape[1]\n",
    "\n",
    "        # encoder\n",
    "        self.encoder.to(DEVICE)\n",
    "        self.decoder.to(DEVICE)\n",
    "        mu, logvar = self.encoder(x)\n",
    "        q_zgivenxobs = td.Independent(td.Normal(loc=mu, scale=logvar), 1)\n",
    "\n",
    "        zgivenx = q_zgivenxobs.rsample([L])\n",
    "        zgivenx_flat = zgivenx.reshape([L * batch_size, self.latent_size])\n",
    "\n",
    "        # decoder\n",
    "        out_decoder = self.decoder(zgivenx_flat)\n",
    "        recon_x_means = self.decoder.l_out_mu(out_decoder)\n",
    "\n",
    "        # loss\n",
    "        data_flat = torch.Tensor.repeat(x, [L, 1]).reshape([-1, 1]).to(DEVICE)\n",
    "        tiledmask = torch.Tensor.repeat(mask, [L, 1]).to(DEVICE)\n",
    "\n",
    "        all_log_pxgivenz_flat = self.decoder.dist_xgivenz(out_decoder, flat=True).log_prob(data_flat)\n",
    "        all_log_pxgivenz = all_log_pxgivenz_flat.reshape([L * batch_size, p])\n",
    "\n",
    "        logpxobsgivenz = torch.sum(all_log_pxgivenz * tiledmask, 1).reshape([L, batch_size])\n",
    "        logpz = self.p_z.log_prob(zgivenx)\n",
    "        logq = q_zgivenxobs.log_prob(zgivenx)\n",
    "\n",
    "        # imputation weighted samples\n",
    "        imp_weights = torch.nn.functional.softmax(\n",
    "            logpxobsgivenz + logpz - logq, 0\n",
    "        )  # these are w_1,....,w_L for all observations in the batch\n",
    "\n",
    "        xgivenz = self.decoder.imp_dist_xgivenz(out_decoder)\n",
    "        xms = xgivenz.sample().reshape([L, batch_size, p])\n",
    "        xm = torch.einsum(\"ki,kij->ij\", imp_weights, xms)\n",
    "\n",
    "        # merge imputed values with observed values\n",
    "        xhat = torch.clone(x)\n",
    "        xhat[~mask.bool()] = xm[~mask.bool()]\n",
    "\n",
    "        return xhat\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T19:15:09.269313Z",
     "start_time": "2024-04-03T19:15:09.261312Z"
    },
    "collapsed": false
   },
   "source": [
    "from copy import deepcopy\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def train(model, X, mask, train_params, X_true, return_params = False):\n",
    "    \n",
    "    model.to(DEVICE)\n",
    "    lr = train_params['lr']\n",
    "    weight_decay = train_params['weight_decay']\n",
    "    epochs = train_params['epochs']\n",
    "    batch_size = train_params['batch_size']\n",
    "    verbose = train_params['verbose']\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay, amsgrad=True)\n",
    "\n",
    "    # data\n",
    "    n = X.shape[0]\n",
    "    X_imp = X.copy()\n",
    "    X_mask = mask.copy()\n",
    "    bs = min(batch_size, n)\n",
    "\n",
    "    final_loss = 0\n",
    "    rmses = []\n",
    "    for ep in range(epochs):\n",
    "        \n",
    "        # evaluation\n",
    "        with torch.no_grad():\n",
    "            X_imp_new = model.impute(\n",
    "                torch.from_numpy(X_imp).float().to(DEVICE), torch.from_numpy(~X_mask).float().to(DEVICE)\n",
    "            )\n",
    "            rmse_value = rmse(X_imp_new.detach().clone().cpu().numpy(), X_true, X_mask)\n",
    "            rmses.append(rmse_value)\n",
    "\n",
    "        # shuffle data\n",
    "        perm = np.random.permutation(n)  # We use the \"random reshuffling\" version of SGD\n",
    "        batches_data = np.array_split(X_imp[perm,], int(n / bs), )\n",
    "        batches_mask = np.array_split(X_mask[perm,], int(n / bs), )\n",
    "        total_loss, total_iters = 0, 0\n",
    "        total_mask_loss = 0\n",
    "        model.train()\n",
    "        for it in range(len(batches_data)):\n",
    "            optimizer.zero_grad()\n",
    "            model.encoder.zero_grad()\n",
    "            model.decoder.zero_grad()\n",
    "            model.mask_net.zero_grad()\n",
    "            b_data = torch.from_numpy(batches_data[it]).float().to(DEVICE)\n",
    "            b_mask = torch.from_numpy(~batches_mask[it]).float().to(DEVICE)\n",
    "            data = [b_data, b_mask]\n",
    "            loss, ret_dict = model.compute_loss(data)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_iters += 1\n",
    "\n",
    "\n",
    "        # print loss\n",
    "        if (ep + 1) % verbose == 0:\n",
    "            print('Epoch %s/%s, Loss = %s RMSE = %s' % (ep, epochs, total_loss / total_iters, rmses[-1]))\n",
    "\n",
    "        if DEVICE == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        final_loss = total_loss / total_iters\n",
    "\n",
    "    model.to(\"cpu\")\n",
    "    \n",
    "    if return_params:\n",
    "        return deepcopy(model.state_dict())\n",
    "    else:\n",
    "        return model\n",
    "\n",
    "def impute(model, X, mask):\n",
    "    model.to(DEVICE)\n",
    "    X_imp = X.copy()\n",
    "    X_mask = mask.copy()\n",
    "    X_imp[mask] = 0\n",
    "    X_imp = torch.from_numpy(X_imp).float().to(DEVICE)\n",
    "    X_mask = torch.from_numpy(~X_mask).float().to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        X_imp_new = model.impute(X_imp, X_mask)\n",
    "    model.to(\"cpu\")\n",
    "    return X_imp_new.detach().clone().cpu().numpy()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T19:15:11.663493Z",
     "start_time": "2024-04-03T19:15:11.644494Z"
    },
    "collapsed": false
   },
   "source": [
    "def train_mask_net(model, X, mask, train_params):\n",
    "    \n",
    "    model.to(DEVICE)\n",
    "    lr = train_params['lr']\n",
    "    weight_decay = train_params['weight_decay']\n",
    "    epochs = train_params['epochs']\n",
    "    batch_size = train_params['batch_size']\n",
    "    verbose = train_params['verbose']\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay, amsgrad=True)\n",
    "\n",
    "    # data\n",
    "    n = X.shape[0]\n",
    "    X_imp = X.copy()\n",
    "    X_mask = mask.copy()\n",
    "    bs = min(batch_size, n)\n",
    "\n",
    "    final_loss = 0\n",
    "    rmses = []\n",
    "    for ep in range(epochs):\n",
    "\n",
    "        # shuffle data\n",
    "        perm = np.random.permutation(n)  # We use the \"random reshuffling\" version of SGD\n",
    "        batches_data = np.array_split(X_imp[perm,], int(n / bs), )\n",
    "        batches_mask = np.array_split(X_mask[perm,], int(n / bs), )\n",
    "        total_loss, total_iters = 0, 0\n",
    "        total_mask_loss = 0\n",
    "        model.train()\n",
    "        for it in range(len(batches_data)):\n",
    "            optimizer.zero_grad()\n",
    "            model.mask_net.zero_grad()\n",
    "            b_data = torch.from_numpy(batches_data[it]).float().to(DEVICE)\n",
    "            b_mask = torch.from_numpy(batches_mask[it]).float().to(DEVICE)\n",
    "            loss = model.mask_prediction_loss(b_data, b_mask)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_iters += 1\n",
    "\n",
    "\n",
    "        # print loss\n",
    "        if (ep + 1) % verbose == 0:\n",
    "            print('Epoch %s/%s, Mask Loss = %s' % (ep, epochs, total_loss / total_iters))\n",
    "\n",
    "        if DEVICE == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        final_loss = total_loss / total_iters\n",
    "\n",
    "    model.to(\"cpu\")\n",
    "    \n",
    "    return model"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T17:34:20.478936Z",
     "start_time": "2024-04-03T17:33:50.397225Z"
    },
    "collapsed": false
   },
   "source": [
    "imp_model1 = MIWAE(\n",
    "    num_features=X1.shape[1], latent_size = 3, n_hidden = 16, n_hidden_layers = 2, out_dist = 'gaussian', K = 20, L = 100\n",
    ")\n",
    "train_params = {\n",
    "    'lr': 1e-3,\n",
    "    'weight_decay': 1e-5,\n",
    "    'epochs': 500,\n",
    "    'batch_size': 64,\n",
    "    'verbose': 10\n",
    "}\n",
    "\n",
    "imp_model1 = train(imp_model1, X_imp1, mask1, train_params, X)\n",
    "X_imp_new1 = impute(imp_model1, X_imp1, mask1)\n",
    "print(X_imp_new1.shape)\n",
    "from src.evaluation.imp_quality_metrics import rmse\n",
    "print(rmse(X_imp_new1, X1, mask1))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": false
   },
   "source": [
    "imp_model2 = MIWAE(\n",
    "    num_features=X2.shape[1], latent_size = 3, n_hidden = 16, n_hidden_layers = 2, out_dist = 'gaussian', K = 20, L = 100\n",
    ")\n",
    "train_params = {\n",
    "    'lr': 1e-3,\n",
    "    'weight_decay': 1e-5,\n",
    "    'epochs': 500,\n",
    "    'batch_size': 64,\n",
    "    'verbose': 10\n",
    "}\n",
    "\n",
    "imp_model2 = train(imp_model2, X_imp2, mask2, train_params, X2)\n",
    "\n",
    "X_imp_new2 = impute(imp_model2, X_imp2, mask2)\n",
    "print(X_imp_new2.shape)\n",
    "from src.evaluation.imp_quality_metrics import rmse\n",
    "print(rmse(X_imp_new2, X2, mask2))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T15:02:40.809375Z",
     "start_time": "2024-04-03T15:02:40.795349Z"
    },
    "collapsed": false
   },
   "source": [
    "X_imp_new1 = impute(imp_model2, X_imp1, mask1)\n",
    "print(X_imp_new1.shape)\n",
    "from src.evaluation.imp_quality_metrics import rmse\n",
    "print(rmse(X_imp_new1, X1, mask1))\n",
    "\n",
    "X_imp_new2 = impute(imp_model1, X_imp2, mask2)\n",
    "print(X_imp_new2.shape)\n",
    "from src.evaluation.imp_quality_metrics import rmse\n",
    "print(rmse(X_imp_new2, X2, mask2))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T19:17:17.071943Z",
     "start_time": "2024-04-03T19:15:15.968381Z"
    },
    "collapsed": false
   },
   "source": [
    "def fedavg(local_model_parameters):\n",
    "    # federated averaging implementation\n",
    "    averaged_model_state_dict = OrderedDict()  # global parameters\n",
    "    #sample_sizes = [item['sample_size'] for item in fit_res]\n",
    "\n",
    "    for it, local_model_state_dict in enumerate(local_model_parameters):\n",
    "        for key in local_model_state_dict.keys():\n",
    "            if it == 0:\n",
    "                averaged_model_state_dict[key] = (1/len(local_model_parameters))*local_model_state_dict[key]\n",
    "            else:\n",
    "                averaged_model_state_dict[key] += (1/len(local_model_parameters))*local_model_state_dict[key]\n",
    "\n",
    "    # copy parameters for each client\n",
    "    agg_model_parameters = [deepcopy(averaged_model_state_dict) for _ in range(len(local_model_parameters))]\n",
    "    agg_res = {}\n",
    "\n",
    "    return agg_model_parameters, agg_res\n",
    "\n",
    "imp_model_global = MIWAE(\n",
    "    num_features=X.shape[1], latent_size = 3, n_hidden = 16, n_hidden_layers = 2, out_dist = 'gaussian', K = 20, L = 100\n",
    ")\n",
    "imp_model1 = MIWAE(\n",
    "    num_features=X1.shape[1], latent_size = 3, n_hidden = 16, n_hidden_layers = 2, out_dist = 'gaussian', K = 20, L = 100\n",
    ")\n",
    "\n",
    "imp_model2 = MIWAE(\n",
    "    num_features=X2.shape[1], latent_size = 3, n_hidden = 16, n_hidden_layers = 2, out_dist = 'gaussian', K = 20, L = 100\n",
    ")\n",
    "\n",
    "imp_model1.load_state_dict(deepcopy(imp_model_global.state_dict()))\n",
    "imp_model2.load_state_dict(deepcopy(imp_model_global.state_dict()))\n",
    "\n",
    "global_iteration = 100\n",
    "\n",
    "train_params = {\n",
    "    'lr': 1e-3,\n",
    "    'weight_decay': 1e-5,\n",
    "    'epochs': 5,\n",
    "    'batch_size': 64,\n",
    "    'verbose': 10\n",
    "}\n",
    "\n",
    "for it in range(global_iteration):\n",
    "    print(f\"Global iteration {it}\")\n",
    "    # global model training\n",
    "    params1 = train(imp_model_global, X_imp1, mask1, train_params, X1, return_params=True)\n",
    "    params2 = train(imp_model_global, X_imp2, mask2, train_params, X2, return_params=True)\n",
    "    \n",
    "    # local model training\n",
    "    imp_model1 = train(imp_model1, X_imp1, mask1, train_params, X1)\n",
    "    imp_model2 = train(imp_model2, X_imp2, mask2, train_params, X2)\n",
    "    \n",
    "    # federated averaging\n",
    "    avg_params, _ = fedavg([params1, params2])\n",
    "    imp_model_global.load_state_dict(avg_params[0])\n",
    "    \n",
    "    # update local model encoder\n",
    "    imp_model1.encoder.load_state_dict(imp_model_global.encoder.state_dict(), strict = False)\n",
    "    imp_model2.encoder.load_state_dict(imp_model_global.encoder.state_dict(), strict = False)\n",
    "    \n",
    "    X_imp_g1 = impute(imp_model_global, X_imp1, mask1)\n",
    "    X_imp_l1 = impute(imp_model1, X_imp1, mask1)\n",
    "    X_imp_g2 = impute(imp_model_global, X_imp2, mask2)\n",
    "    X_imp_l2 = impute(imp_model2, X_imp2, mask2)\n",
    "    print(rmse(X_imp_g1, X1, mask1), rmse(X_imp_g2, X2, mask2))\n",
    "    print(rmse(X_imp_l1, X1, mask1), rmse(X_imp_l2, X2, mask2))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T19:17:48.031626Z",
     "start_time": "2024-04-03T19:17:44.926723Z"
    },
    "collapsed": false
   },
   "source": [
    "train_params = {\n",
    "    'lr': 1e-3,\n",
    "    'weight_decay': 1e-5,\n",
    "    'epochs': 60,\n",
    "    'batch_size': 64,\n",
    "    'verbose': 10\n",
    "}\n",
    "\n",
    "imp_model1 = train_mask_net(imp_model1, X_imp1, mask1, train_params)\n",
    "imp_model2 = train_mask_net(imp_model2, X_imp2, mask2, train_params)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T19:52:29.920685Z",
     "start_time": "2024-04-03T19:52:29.907685Z"
    },
    "collapsed": false
   },
   "source": [
    "import torch.distributions as td\n",
    "\n",
    "def new_impute(global_model, local_model, x: torch.Tensor, mask: torch.Tensor, L) -> torch.Tensor:\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        x = x.to(DEVICE)\n",
    "        mask = ~mask.to(DEVICE)\n",
    "        batch_size = x.shape[0]\n",
    "        p = x.shape[1]\n",
    "        \n",
    "        # encoder\n",
    "        global_model.encoder.to(DEVICE)\n",
    "        global_model.decoder.to(DEVICE)\n",
    "        local_model.encoder.to(DEVICE)\n",
    "        local_model.decoder.to(DEVICE)\n",
    "        local_model.mask_net.to(DEVICE)\n",
    "        mu, logvar = local_model.encoder(x)\n",
    "        \n",
    "        q_zgivenxobs = td.Independent(td.Normal(loc=mu, scale=logvar), 1)\n",
    "        zgivenx = q_zgivenxobs.rsample([L])\n",
    "        zgivenx_flat = zgivenx.reshape([L * batch_size, local_model.latent_size])\n",
    "    \n",
    "        # decoder\n",
    "        global_out_decoder = global_model.decoder(zgivenx_flat)\n",
    "        local_out_decoder = local_model.decoder(zgivenx_flat)\n",
    "        #recon_x_means = self.decoder.l_out_mu(out_decoder)\n",
    "    \n",
    "        # loss\n",
    "        data_flat = torch.Tensor.repeat(x, [L, 1]).reshape([-1, 1]).to(DEVICE)\n",
    "        tiledmask = torch.Tensor.repeat(mask, [L, 1]).to(DEVICE)\n",
    "    \n",
    "        all_log_pxgivenz_flat_local = local_model.decoder.dist_xgivenz(local_out_decoder, flat=True).log_prob(data_flat)\n",
    "        all_log_pxgivenz_local = all_log_pxgivenz_flat_local.reshape([L * batch_size, p])\n",
    "        logpxobsgivenz = torch.sum(all_log_pxgivenz_local * tiledmask, 1).reshape([L, batch_size])\n",
    "        \n",
    "        all_log_pxgivenz_flat_global = global_model.decoder.dist_xgivenz(global_out_decoder, flat=True).log_prob(data_flat)\n",
    "        all_log_pxgivenz_global = all_log_pxgivenz_flat_global.reshape([L * batch_size, p])\n",
    "        logpxobsgivenz_global = torch.sum(all_log_pxgivenz_global * tiledmask, 1).reshape([L, batch_size])\n",
    "        \n",
    "        #print(all_log_pxgivenz_global.exp(), all_log_pxgivenz_local.exp())\n",
    "        \n",
    "        logpz = local_model.p_z.log_prob(zgivenx)\n",
    "        logq = q_zgivenxobs.log_prob(zgivenx)\n",
    "        \n",
    "        logpz_global = global_model.p_z.log_prob(zgivenx)\n",
    "        logq_global = q_zgivenxobs.log_prob(zgivenx)\n",
    "            \n",
    "        # imputation weighted samples\n",
    "        imp_weights_local = torch.nn.functional.softmax(\n",
    "            logpxobsgivenz + logpz - logq, 0\n",
    "        )  # these are w_1,....,w_L for all observations in the batch\n",
    "        imp_weights_global = torch.nn.functional.softmax(\n",
    "            logpxobsgivenz_global + logpz_global - logq_global, 0\n",
    "        )\n",
    "    \n",
    "        xgivenz_local = local_model.decoder.imp_dist_xgivenz(local_out_decoder)\n",
    "        xgivenz_global = global_model.decoder.imp_dist_xgivenz(global_out_decoder)\n",
    "        xms_local = xgivenz_local.sample().reshape([L, batch_size, p])\n",
    "        xms_global = xgivenz_global.sample().reshape([L, batch_size, p])\n",
    "        \n",
    "        mask_prob = local_model.mask_net(zgivenx_flat)\n",
    "        mask_prob = mask_prob.reshape([L, batch_size, p]) + 0.01  # avoid zero\n",
    "        \n",
    "        xm_local = torch.einsum(\"ki,kij->ij\", imp_weights_local, (1 - 1/mask_prob)*xms_local)\n",
    "        xm_global = torch.einsum(\"ki,kij->ij\", imp_weights_local, 1/mask_prob*xms_global)\n",
    "        \n",
    "        xm = xm_global + xm_local\n",
    "    \n",
    "        # merge imputed values with observed values\n",
    "        xhat = torch.clone(x)\n",
    "        xhat[~mask.bool()] = xm[~mask.bool()]\n",
    "    \n",
    "        return xhat.detach().clone().cpu().numpy()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T19:52:30.978895Z",
     "start_time": "2024-04-03T19:52:30.721368Z"
    },
    "collapsed": false
   },
   "source": [
    "X_imp_new1 = new_impute(imp_model_global, imp_model1, torch.from_numpy(X_imp1).float(), torch.from_numpy(mask1), 1000)\n",
    "print(rmse(X_imp_new1, X1, mask1))\n",
    "\n",
    "X_imp_new2 = new_impute(imp_model_global, imp_model2, torch.from_numpy(X_imp2).float(), torch.from_numpy(mask2), 1000)\n",
    "print(rmse(X_imp_new2, X2, mask2))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T19:17:55.244987Z",
     "start_time": "2024-04-03T19:17:55.226987Z"
    },
    "collapsed": false
   },
   "source": [
    "X_imp_new1 = new_impute(imp_model_global, imp_model1, torch.from_numpy(X_imp1[-4:,:]).float(), torch.from_numpy(mask1[-4:,:]), 5)\n",
    "X_imp_g1 = impute(imp_model_global, X_imp1[-4:,:], mask1[-4:,:])\n",
    "X_imp_l1 = impute(imp_model1, X_imp1[-4:,:], mask1[-4:,:])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T19:17:57.017192Z",
     "start_time": "2024-04-03T19:17:57.009193Z"
    },
    "collapsed": false
   },
   "source": [
    "X[-4:]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T17:50:44.618840Z",
     "start_time": "2024-04-03T17:50:44.604114Z"
    },
    "collapsed": false
   },
   "source": [
    "X_imp_new1"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T15:46:16.122178Z",
     "start_time": "2024-04-03T15:46:16.103621Z"
    },
    "collapsed": false
   },
   "source": [
    "X_imp_g1"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T17:50:47.126881Z",
     "start_time": "2024-04-03T17:50:47.114820Z"
    },
    "collapsed": false
   },
   "source": [
    "X_imp_l1"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fed_imp2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
