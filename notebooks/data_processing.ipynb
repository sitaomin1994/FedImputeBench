{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "cd .."
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b26046a",
   "metadata": {},
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext dotenv\n",
    "%dotenv\n",
    "import pandas as pd\n",
    "from src.loaders.load_data import load_data\n",
    "from collections import OrderedDict\n",
    "import numpy as np"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e54e941e",
   "metadata": {},
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "X = np.random.choice([0, 1], size=(100, 1))\n",
    "est = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='uniform', subsample = None)\n",
    "est.fit_transform(X)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ad4ad0",
   "metadata": {},
   "source": [
    "target_processing_strategies = {\n",
    "    'school_pca': {\n",
    "        'min': 0\n",
    "        'max': 70,\n",
    "        'group_idx': [10, 20, 30, 40, 50 , 60]\n",
    "    },\n",
    "    'dvisits': {\n",
    "        'min': 0,\n",
    "        'max': 8,\n",
    "        'group_idx': [1, 4]   \n",
    "    }, \n",
    "    'california': {\n",
    "        'min': 0,\n",
    "        'max': 5,\n",
    "        'group_idx': None\n",
    "    },\n",
    "    'hhip': {\n",
    "        'min': 0,\n",
    "        'max': 15,\n",
    "        'group_idx': [2, 3, 4, 5, 6] \n",
    "    }\n",
    "}\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "29962f1a",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff9a9f4a",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def show_heatmap(df, figsize=(8, 6)):\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(df.corr(), annot=True, fmt=\".1f\")\n",
    "    plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c065f7f0",
   "metadata": {},
   "source": [
    "def avg_correlation(df):\n",
    "    avg_correlation_cols = list(OrderedDict(df.corr().abs().mean().sort_values(ascending=False).to_dict()).items())\n",
    "    features = set(df.columns.tolist()[:-1])\n",
    "    avg_correlation_cols = [col for col in avg_correlation_cols if col[0] in features]\n",
    "    return avg_correlation_cols"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9707b1d5",
   "metadata": {},
   "source": [
    "from sklearn.cluster import KMeans\n",
    "def kbins(data, data_config):\n",
    "\n",
    "    est = KMeans(n_clusters=10, random_state=0)\n",
    "\n",
    "    X = data.iloc[:, data_config['split_col_idx']].values\n",
    "    X = est.fit_predict(X)\n",
    "    \n",
    "    return X"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "494b7397",
   "metadata": {},
   "source": [
    "## Codrna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f594643",
   "metadata": {},
   "source": [
    "df, data_config = load_data('codrna')\n",
    "show_heatmap(df,  figsize = (6,4))\n",
    "print(avg_correlation(df))\n",
    "print(data_config)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "abd0da75",
   "metadata": {},
   "source": [
    "avg_cols = avg_correlation(df)\n",
    "avg_cols = [col[0] for col in avg_cols]\n",
    "split_col_idx = [df.columns.tolist().index(col) for col in avg_cols]\n",
    "split_col_idx"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c881b23c",
   "metadata": {},
   "source": [
    "cl = kbins(df, data_config)\n",
    "np.unique(cl, return_counts=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e0752c46",
   "metadata": {},
   "source": [
    "data_config"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "c9b14ce9",
   "metadata": {},
   "source": [
    "## HHP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14b452c6",
   "metadata": {},
   "source": [
    "\n",
    "############################################################################################################\n",
    "# Load data\n",
    "# members\n",
    "df_members = pd.read_csv('./data/HHP_herritage_health/Members.csv')\n",
    "df_members['Sex'] = df_members['Sex'].map({'M': 1, 'F': 0})\n",
    "df_members['AgeAtFirstClaim'] = df_members['AgeAtFirstClaim'].map({\n",
    "    '0-9': 5,\n",
    "    '10-19': 15,\n",
    "    '20-29': 25,\n",
    "    '30-39': 35,\n",
    "    '40-49': 45,\n",
    "    '50-59': 55,\n",
    "    '60-69': 65,\n",
    "    '70-79': 75,\n",
    "    '80+': 90\n",
    "})\n",
    "\n",
    "# drug and lab\n",
    "df_drug = pd.read_csv('./data/HHP_herritage_health/DrugCount.csv')\n",
    "df_drug['DrugCount'] = df_drug['DrugCount'].map({'1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7+': 10})\n",
    "df_lab = pd.read_csv('./data/HHP_herritage_health/LabCount.csv')\n",
    "df_lab['LabCount'] = df_lab['LabCount'].map(\n",
    "    {'1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9, '10+': 12}\n",
    ")\n",
    "\n",
    "# days\n",
    "df_days_y2 = pd.read_csv('./data/HHP_herritage_health/DaysInHospital_Y2.csv')\n",
    "df_days_y2['Year'] = 'Y1'\n",
    "df_days_y3 = pd.read_csv('./data/HHP_herritage_health/DaysInHospital_Y3.csv')\n",
    "df_days_y3['Year'] = 'Y2'\n",
    "\n",
    "df_days = pd.concat([df_days_y2, df_days_y3])\n",
    "\n",
    "df_claims = pd.read_csv('./data/HHP_herritage_health/Claims.csv')\n",
    "\n",
    "# divide by provider\n",
    "# providers = df_claims['ProviderID'].value_counts().iloc[: 10].index.tolist()\n",
    "# df_claims = df_claims[df_claims['ProviderID'].isin(providers)]\n",
    "# print(df_claims.shape)\n",
    "\n",
    "df_claims = pd.merge(df_claims, df_members, on='MemberID', how='left')\n",
    "df_claims = pd.merge(df_claims, df_drug, on=['MemberID', 'Year', 'DSFS'], how='left')\n",
    "df_claims = pd.merge(df_claims, df_lab, on=['MemberID', 'Year', 'DSFS'], how='left')\n",
    "df_claims = pd.merge(df_claims, df_days, on=['MemberID', 'Year'], how='left')\n",
    "df_claims = df_claims[df_claims['DaysInHospital'].notna()]\n",
    "print(df_claims.shape)\n",
    "\n",
    "#################################################################################################################\n",
    "# Feature engineering\n",
    "# drop missing age and sex\n",
    "df_claims = df_claims[df_claims['AgeAtFirstClaim'].notna()].copy()\n",
    "df_claims = df_claims[df_claims['Sex'].notna()].copy()\n",
    "df_claims = df_claims[df_claims['DSFS'].notna()].copy()\n",
    "\n",
    "# transform categorical columns\n",
    "def transform1(row):\n",
    "    if pd.isna(row):\n",
    "        return 'None'\n",
    "    else:\n",
    "        return str(int(row))\n",
    "    \n",
    "df_claims['ProviderID'] = df_claims['ProviderID'].map(transform1)\n",
    "df_claims['Vendor'] = df_claims['Vendor'].map(transform1)\n",
    "df_claims['PCP'] = df_claims['PCP'].map(transform1)\n",
    "\n",
    "# handle missing values for categorical columns\n",
    "df_claims['ProcedureGroup'] = df_claims['ProcedureGroup'].fillna('None', inplace=False)\n",
    "df_claims['Specialty'] = df_claims['Specialty'].fillna('None', inplace=False)\n",
    "df_claims['PrimaryConditionGroup'] = df_claims['PrimaryConditionGroup'].fillna('None', inplace=False)\n",
    "df_claims['PlaceSvc'] = df_claims['PlaceSvc'].fillna('None', inplace=False)\n",
    "\n",
    "# encode number of columns\n",
    "df_claims['CharlsonIndex'] = df_claims['CharlsonIndex'].map({'0': 0, '1-2': 1.5, '3-4': 3.5, '5+': 7})\n",
    "df_claims['PayDelay'] = df_claims['PayDelay'].apply(lambda row: int(row) if row != '162+' else 200)\n",
    "df_claims['LengthOfStay'] = df_claims['LengthOfStay'].map({\n",
    "    '1 day': 1, '2 days': 2, '3 days': 3, '4 days': 4, '5 days': 5, '6 days': 6, '1- 2 weeks': 10, '2- 4 weeks': 21, '4- 8 weeks': 42,\n",
    "})\n",
    "df_claims['DSFS'] = df_claims['DSFS'].map({\n",
    "    '0- 1 month': 1, '1- 2 months': 2, '2- 3 months': 3, '3- 4 months': 4, '4- 5 months': 5, '5- 6 months': 6, \n",
    "    '6- 7 months': 7, '7- 8 months': 8, '8- 9 months': 9, '9-10 months': 10, '10-11 months': 11, '11-12 months': 12\n",
    "})\n",
    "\n",
    "# filter all large claims\n",
    "df_claims = df_claims[df_claims['DaysInHospital'] > 0].copy()\n",
    "print(df_claims.shape)\n",
    "\n",
    "# fill mean values for drug and lab counts\n",
    "df_claims['DrugCount'] = df_claims['DrugCount'].fillna(df_claims['DrugCount'].mean(), inplace=False)\n",
    "df_claims['LabCount'] = df_claims['LabCount'].fillna(df_claims['LabCount'].mean(), inplace=False)\n",
    "\n",
    "# drop length of stay\n",
    "df_claims = df_claims.drop(columns=['LengthOfStay'])\n",
    "\n",
    "#########################################################################################################################\n",
    "# Feature selection\n",
    "# numerical features\n",
    "def feature_agg(df, key):\n",
    "    ret = df.groupby(['MemberID', 'Year']).agg(\n",
    "        **{\n",
    "            key+'_mean': pd.NamedAgg(column=key, aggfunc='mean'),\n",
    "            key+'_std': pd.NamedAgg(column=key, aggfunc='std'),\n",
    "            key+'_max': pd.NamedAgg(column=key, aggfunc='max'),\n",
    "            key+'_min': pd.NamedAgg(column=key, aggfunc='min'),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    ret[f'{key}_range'] = ret[f'{key}_max'] - ret[f'{key}_min']\n",
    "    ret = ret.reset_index()\n",
    "    ret = ret.drop(columns=[f'{key}_min'])\n",
    "    \n",
    "    df = pd.merge(df, ret, on=['MemberID', 'Year'], how='left')\n",
    "    df = df.drop(columns=[key])\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_claims = feature_agg(df_claims, 'DSFS')\n",
    "df_claims = feature_agg(df_claims, 'PayDelay')\n",
    "df_claims = feature_agg(df_claims, 'CharlsonIndex')\n",
    "df_claims = feature_agg(df_claims, 'DrugCount')\n",
    "df_claims = feature_agg(df_claims, 'LabCount')\n",
    "\n",
    "def feature_agg2(df, key):\n",
    "    df = df.groupby(['MemberID', 'Year']).agg(key).nunique().reset_index(name = key+'_counts')\n",
    "    df = pd.merge(df_claims, df, on = ['MemberID', 'Year'], how = 'left')\n",
    "    return df\n",
    "\n",
    "df_claims = feature_agg2(df_claims, 'ProviderID')\n",
    "df_claims = feature_agg2(df_claims, 'Vendor')\n",
    "df_claims = feature_agg2(df_claims, 'PCP')\n",
    "df_claims = feature_agg2(df_claims, 'Specialty')\n",
    "df_claims = feature_agg2(df_claims, 'PlaceSvc')\n",
    "df_claims = feature_agg2(df_claims, 'PrimaryConditionGroup')\n",
    "df_claims = feature_agg2(df_claims, 'ProcedureGroup')\n",
    "ret = df_claims.groupby(['MemberID', 'Year']).size().reset_index(name = 'claim_counts')\n",
    "df_claims = pd.merge(df_claims, ret, on = ['MemberID', 'Year'], how = 'left')\n",
    "\n",
    "df_claims.fillna(0, inplace=True)\n",
    "\n",
    "# categoorical one-hot features\n",
    "top_k = 2\n",
    "for col in ['Specialty', 'PlaceSvc', 'PrimaryConditionGroup', 'ProcedureGroup']:\n",
    "    top_k_cols = pd.get_dummies(df_claims[col]).corrwith(df_claims['DaysInHospital']).abs().sort_values(ascending = False)[:top_k]\n",
    "    dummies = pd.get_dummies(df_claims[col])[top_k_cols.index]\n",
    "    dummies.columns = [f'{col}_{idx}' for idx in range(len(dummies.columns))]\n",
    "    df_claims = pd.concat([df_claims, dummies], axis = 1)\n",
    "    df_claims.drop(columns = [col], inplace = True)\n",
    "    \n",
    "df_claims = df_claims.drop(columns = ['MemberID', 'ProviderID', 'Vendor', 'PCP', 'Year'])\n",
    "\n",
    "#########################################################################################################################\n",
    "# Split data\n",
    "# columns\n",
    "num_cols = ['AgeAtFirstClaim']\n",
    "for col in ['CharlsonIndex', 'PayDelay', 'DrugCount', 'LabCount', 'DSFS']:\n",
    "    num_cols += [f'{col}_mean', f'{col}_std', f'{col}_max', f'{col}_range']\n",
    "for col in ['ProviderID', 'Vendor', 'PCP', 'Specialty', 'PlaceSvc', 'PrimaryConditionGroup', 'ProcedureGroup', 'claim']:\n",
    "    num_cols += [f'{col}_counts']\n",
    "print(len(num_cols))\n",
    "\n",
    "cat_cols = ['Sex', 'SupLOS', 'ClaimsTruncated']\n",
    "cat_cols += [f'{col}_{idx}' for col in ['Specialty', 'PlaceSvc', 'PrimaryConditionGroup', 'ProcedureGroup'] for idx in range(top_k)]\n",
    "print(len(cat_cols))\n",
    "\n",
    "target = 'DaysInHospital'\n",
    "\n",
    "# sample data\n",
    "df_claims_sample = df_claims.sample(n = 20000, random_state=42)\n",
    "\n",
    "# standardize\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df_claims_sample[num_cols] = scaler.fit_transform(df_claims_sample[num_cols])\n",
    "scaler = MinMaxScaler()\n",
    "df_claims_sample[num_cols] = scaler.fit_transform(df_claims_sample[num_cols])\n",
    "\n",
    "# reorder target to be num cols, cat cols and target\n",
    "df_claims_sample = df_claims_sample[num_cols + cat_cols + [target]]\n",
    "\n",
    "print(df_claims_sample.shape)\n",
    "\n",
    "data = df_claims_sample\n",
    "\n",
    "avg_correlation_cols = avg_correlation(data)\n",
    "avg_correlation_cols = [col[0] for col in avg_correlation_cols if col[0] in num_cols]\n",
    "avg_correlation_cols = [col for col in avg_correlation_cols][:int(data.shape[1]*0.3)]\n",
    "\n",
    "# data config\n",
    "data_config = {\n",
    "    'target': target,\n",
    "    'features_idx': [idx for idx in range(0, data.shape[1]) if data.columns[idx] != target],\n",
    "    'split_col_idx': [data.columns.tolist().index(col) for col in avg_correlation_cols],\n",
    "    'ms_col_idx': [idx for idx in range(0, data.shape[1]) if data.columns[idx] in num_cols],\n",
    "    'obs_col_idx': [idx for idx in range(0, data.shape[1]) if data.columns[idx] in cat_cols],\n",
    "    \"num_cols\": len(num_cols),\n",
    "    'task_type': 'regression',\n",
    "    'clf_type': 'none',\n",
    "    'data_type': 'tabular'\n",
    "}"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a0f53d5",
   "metadata": {},
   "source": [
    "data[target].hist(bins = 100)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "22ce38ce",
   "metadata": {},
   "source": [
    "data_config"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "486b3b6a",
   "metadata": {},
   "source": [
    "import json\n",
    "data.to_csv('./data/HHP_herritage_health/data_cleaned.csv', index=False)\n",
    "with open('./data/HHP_herritage_health/data_config.json', 'w') as f:\n",
    "    json.dump(data_config, f)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "1ce70ef0",
   "metadata": {},
   "source": [
    "data.shape"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "8fab1d47",
   "metadata": {},
   "source": [
    "data_config['split_col_idx']"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "90cf2ce2",
   "metadata": {},
   "source": [
    "avg_correlation(data[num_cols])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "98a1da0f",
   "metadata": {},
   "source": [
    "data[num_cols].corrwith(data['DaysInHospital']).abs().sort_values(ascending = False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b355ac6b",
   "metadata": {},
   "source": [
    "data_config"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "33b3b1bb",
   "metadata": {},
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_claims_sample[num_cols]\n",
    "y = df_claims_sample[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train.shape, X_test.shape"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6c5ed5b3",
   "metadata": {},
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_squared_log_error\n",
    "\n",
    "model = RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1, 10]).fit(X_train, y_train)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(np.sqrt(mean_squared_error(y_test, y_pred)), np.sqrt(mean_squared_log_error(y_test, y_pred)))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3f0a873d",
   "metadata": {},
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_squared_log_error\n",
    "\n",
    "model = MLPRegressor(hidden_layer_sizes=(128, 128), max_iter=1000, alpha=0.5, random_state=42, verbose=True)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(np.sqrt(mean_squared_error(y_test, y_pred)))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d531c953",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "669e9b59",
   "metadata": {},
   "source": [
    "## California Housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e36ba30b",
   "metadata": {},
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "def outlier_remove_iqr(data, col):\n",
    "    Q1 = data[col].quantile(0.25)\n",
    "    Q3 = data[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    data = data[(data[col] >= (Q1 - 1.5 * IQR)) & (data[col] <= (Q3 + 1.5 * IQR))]\n",
    "    return data\n",
    "\n",
    "def convert_gaussian(data, col):\n",
    "    pt = PowerTransformer()\n",
    "    data[col] = pt.fit_transform(data[col].values.reshape(-1, 1)).flatten()\n",
    "    return data"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7c476ad",
   "metadata": {},
   "source": [
    "from sklearn.datasets import fetch_california_housing, fetch_kddcup99\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import pandas as pd\n",
    "housing = fetch_california_housing()\n",
    "data = pd.DataFrame(data=housing.data, columns=housing.feature_names)\n",
    "target_col = 'MedHouseVal'\n",
    "data[target_col] = housing.target\n",
    "\n",
    "# drop missing values\n",
    "print(data.shape)\n",
    "data = data.dropna()\n",
    "print(data.shape)\n",
    "\n",
    "# remove outliers\n",
    "data = outlier_remove_iqr(data, 'AveRooms')\n",
    "data = outlier_remove_iqr(data, 'AveBedrms')\n",
    "data = outlier_remove_iqr(data, 'Population')\n",
    "data = outlier_remove_iqr(data, 'AveOccup')\n",
    "\n",
    "# gaussian transform\n",
    "data = convert_gaussian(data, 'MedInc')\n",
    "\n",
    "num_cols = data.columns.tolist()[:-1]\n",
    "\n",
    "scaler = Pipeline([\n",
    "    ('standard', StandardScaler()),\n",
    "    ('minmax', MinMaxScaler())\n",
    "])\n",
    "\n",
    "data[num_cols] = scaler.fit_transform(data[num_cols])\n",
    "print(data.shape)\n",
    "\n",
    "data_config = {\n",
    "    'target': target_col,\n",
    "    'features_idx': list(range(len(data.columns)-1)),\n",
    "    'split_col_idx': [0, 2, 5],\n",
    "    'ms_col_idx': list(range(len(num_cols))),\n",
    "    'obs_col_idx': [4, 7],\n",
    "    'num_cols': len(num_cols),\n",
    "    'task_type': 'regression',\n",
    "    'clf_type': 'none',\n",
    "    'data_type': 'tabular'\n",
    "}\n",
    "\n",
    "print(data_config)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c088b77",
   "metadata": {},
   "source": [
    "data.columns"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b1aa99f",
   "metadata": {},
   "source": [
    "data[target_col].hist(bins=100)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f635dfeb",
   "metadata": {},
   "source": [
    "data.to_csv('./data/california/data_cleaned.csv', index=False)\n",
    "import json\n",
    "with open('./data/california/data_config.json', 'w') as f:\n",
    "    json.dump(data_config, f)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "da5543df",
   "metadata": {},
   "source": [
    "data[num_cols].corrwith(data[target_col]).abs().sort_values(ascending = False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f00c220a",
   "metadata": {},
   "source": [
    "avg_correlation(data[num_cols])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d41e0708",
   "metadata": {},
   "source": [
    "show_heatmap(data[num_cols])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bdc81f5c",
   "metadata": {},
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data[num_cols]\n",
    "y = data[target_col]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train.shape, X_test.shape\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_squared_log_error\n",
    "\n",
    "model = MLPRegressor(hidden_layer_sizes=(128, 128), max_iter=1000, alpha=0.5, random_state=42, verbose=True)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(np.sqrt(mean_squared_error(y_test, y_pred)))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cc2cdaeb",
   "metadata": {},
   "source": [
    "## Dvisits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fadd67e6",
   "metadata": {},
   "source": [
    "! pip install pyreadr"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20295f14",
   "metadata": {},
   "source": [
    "def convert_gaussian(data, col):\n",
    "    pt = PowerTransformer()\n",
    "    data[col] = pt.fit_transform(data[col].values.reshape(-1, 1)).flatten()\n",
    "    return data"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94ddd541",
   "metadata": {},
   "source": [
    "import pyreadr\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import pandas as pd\n",
    "result = pyreadr.read_r('./data/dvisits/dvisits.rda')  \n",
    "data = result['dvisits']\n",
    "data = data.drop(['prescrib', 'nonpresc', 'agesq'], axis=1)\n",
    "print(data.shape)\n",
    "\n",
    "num_cols = ['age', 'income', 'illness', 'actdays', 'hscore', 'hospadmi', 'hospdays']\n",
    "cat_cols = ['sex', 'levyplus', 'freepoor', 'freerepa', 'chcond1', 'chcond2']\n",
    "target_col = 'medicine'\n",
    "\n",
    "data = data[num_cols + cat_cols + [target_col]]\n",
    "\n",
    "scaler = Pipeline([\n",
    "    ('standard', StandardScaler()),\n",
    "    ('minmax', MinMaxScaler())\n",
    "])\n",
    "data[num_cols] = scaler.fit_transform(data[num_cols])\n",
    "print(data.shape)\n",
    "\n",
    "data_config = {\n",
    "    'target': target_col,\n",
    "    'features_idx': list(range(len(data.columns)-1)),\n",
    "    'split_col_idx': [0, 2, 1],\n",
    "    'ms_col_idx': [idx for idx in range(0, data.shape[1]) if data.columns[idx] in num_cols],\n",
    "    'obs_col_idx': [idx for idx in range(0, data.shape[1]) if data.columns[idx] in cat_cols],\n",
    "    'num_cols': len(num_cols),\n",
    "    'task_type': 'regression',\n",
    "    'clf_type': 'none',\n",
    "    'data_type': 'tabular'\n",
    "}\n",
    "\n",
    "print(data_config)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ddcdca09",
   "metadata": {},
   "source": [
    "data.hist(bins=30, figsize=(10, 8))\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78309a94",
   "metadata": {},
   "source": [
    "data[target_col].hist(bins=100)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "11ee4795",
   "metadata": {},
   "source": [
    "data.columns"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "902ffdaa",
   "metadata": {},
   "source": [
    "data.to_csv('./data/dvisits/data_cleaned.csv', index=False)\n",
    "import json\n",
    "with open('./data/dvisits/data_config.json', 'w') as f:\n",
    "    json.dump(data_config, f)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f67beed2",
   "metadata": {},
   "source": [
    "data.corrwith(data[target_col]).abs().sort_values(ascending = False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2b45c42c",
   "metadata": {},
   "source": [
    "avg_correlation(data)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "69e3d518",
   "metadata": {},
   "source": [
    "show_heatmap(data.corr().abs())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c28524ea",
   "metadata": {},
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data[cat_cols]\n",
    "y = data[target_col]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train.shape, X_test.shape\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_squared_log_error\n",
    "\n",
    "model = MLPRegressor(hidden_layer_sizes=(128, 128), max_iter=1000, alpha=0.5, random_state=42, verbose=True)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(np.sqrt(mean_squared_error(y_test, y_pred)))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "1175f959",
   "metadata": {},
   "source": [
    "## Vehicle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36e999c2",
   "metadata": {},
   "source": [
    "import scipy.io\n",
    "from collections import Counter"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97b011fe",
   "metadata": {},
   "source": [
    "mat = scipy.io.loadmat('./data/vehicle/vehicle.mat')\n",
    "raw_x, raw_y = mat['X'], mat['Y']  # y = {-1, 1}\n",
    "assert len(raw_x) == len(raw_y)\n",
    "num_clients = len(raw_x)\n",
    "\n",
    "dataset = []\n",
    "for i in range(num_clients):\n",
    "    features, labels = raw_x[i][0], raw_y[i][0].flatten()\n",
    "    print(raw_x[i][0].shape, raw_y[i][0].shape)\n",
    "    assert len(features) == len(labels)\n",
    "    counter = Counter(labels)\n",
    "    print(f'Client {i}:', counter, counter[1] / len(labels))\n",
    "    dataset.append((features, labels))\n",
    "\n",
    "positive_counts = [np.count_nonzero(labels + 1) for feats, labels in dataset]\n",
    "positive_percentages = [np.count_nonzero(labels + 1) / len(labels) * 100\n",
    "                      for feats, labels in dataset]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "059da424",
   "metadata": {},
   "source": [
    "from src.evaluation.imp_quality_metrics import sliced_ws\n",
    "\n",
    "dfs = []\n",
    "for i in range(num_clients):\n",
    "    df = pd.DataFrame(dataset[i][0])\n",
    "    df['label'] = dataset[i][1]\n",
    "    dfs.append(df)\n",
    "\n",
    "final_df = pd.concat(dfs, axis=0)\n",
    "split_indices = np.cumsum([df.shape[0] for df in dfs[:-1]])\n",
    "print(split_indices)\n",
    "print(final_df.shape)\n",
    "final_df"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6eff2ab7",
   "metadata": {},
   "source": [
    "# Number of clients/dataframes\n",
    "num_dfs = len(dfs)\n",
    "distance_matrix = np.zeros((num_dfs, num_dfs))\n",
    "\n",
    "for i in range(num_dfs):\n",
    "    for j in range(i + 1, num_dfs):\n",
    "        # Assuming sliced_ws expects numpy arrays; adapt if necessary\n",
    "        dist = sliced_ws(dfs[i].to_numpy(), dfs[j].to_numpy())\n",
    "        distance_matrix[i, j] = dist\n",
    "        distance_matrix[j, i] = dist\n",
    "\n",
    "# Optional: Fill diagonal with zeros, assuming self-distance is zero\n",
    "np.fill_diagonal(distance_matrix, 0)\n",
    "\n",
    "# Average distance for each dataframe\n",
    "average_distances = np.mean(distance_matrix, axis=1)\n",
    "\n",
    "# Create an array of dataframe indices for reference\n",
    "df_indices = np.arange(num_dfs)\n",
    "\n",
    "# Combine indices and their corresponding average distances\n",
    "average_distances_with_indices = list(zip(df_indices, average_distances))\n",
    "\n",
    "# Sort the list of tuples by the average distance in descending order\n",
    "sorted_by_distance = sorted(average_distances_with_indices, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Select the top 10\n",
    "top_10_different = sorted_by_distance[:10]\n",
    "\n",
    "# Unpack the top 10 list to separate indices and distances for clearer output\n",
    "top_10_indices = [item[0] for item in top_10_different]\n",
    "top_10_distances = [item[1] for item in top_10_different]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "da91ed0a",
   "metadata": {},
   "source": [
    "# feature selection\n",
    "selected_dfs = [dfs[i] for i in top_10_indices]\n",
    "selected_df = pd.concat(selected_dfs, axis=0)\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor  # or RandomForestClassifier based on your task\n",
    "\n",
    "# Assuming 'label' is your prediction target\n",
    "X = selected_df.drop('label', axis=1)\n",
    "y = selected_df['label']\n",
    "\n",
    "model = RandomForestRegressor()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Get feature importances\n",
    "importances = model.feature_importances_\n",
    "feature_names = X.columns\n",
    "feature_importance_dict = dict(zip(feature_names, importances))\n",
    "\n",
    "# Sort features by importance\n",
    "sorted_features = sorted(feature_importance_dict.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "sorted_features = [feature[0] for feature in sorted_features if feature[1] > 0.01]\n",
    "print(sorted_features)\n",
    "\n",
    "correlated_cols = selected_df.corrwith(selected_df['label']).abs().sort_values(ascending = False)[:30].index.tolist()\n",
    "print(correlated_cols)\n",
    "\n",
    "final_cols = list(set(sorted_features) | set(correlated_cols))\n",
    "final_cols.remove('label')\n",
    "print(final_cols)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "974f6029",
   "metadata": {},
   "source": [
    "show_heatmap(selected_df[final_cols + ['label']], figsize=(20, 20))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "07539841",
   "metadata": {},
   "source": [
    "data = selected_df[final_cols]\n",
    "data = data.drop(columns = [41, 42, 43, 37, 44, 47, 46, 49, 45, 48])\n",
    "print(data.shape)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# scaling\n",
    "scaler = Pipeline([\n",
    "    ('standard', StandardScaler()),\n",
    "    ('minmax', MinMaxScaler())\n",
    "])\n",
    "\n",
    "cols = data.columns.tolist()\n",
    "data = scaler.fit_transform(data)\n",
    "data = pd.DataFrame(data, columns = cols)\n",
    "\n",
    "# gaussian transform\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "# Assuming 'df' is your DataFrame with binary features\n",
    "gaussian_cols = [11, 35, 36, 38, 39, 40, 58]\n",
    "transformer = QuantileTransformer(output_distribution='normal', random_state=0)\n",
    "df_gaussianized = transformer.fit_transform(data[gaussian_cols])\n",
    "df_gaussianized = pd.DataFrame(df_gaussianized, columns=gaussian_cols)\n",
    "data = pd.concat([data.drop(columns = gaussian_cols), df_gaussianized], axis = 1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "43060e62",
   "metadata": {},
   "source": [
    "data.hist(figsize=(20, 20), bins = 100)\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4ed78cba",
   "metadata": {},
   "source": [
    "data = selected_df[final_cols]\n",
    "data = data.drop(columns = [41, 42, 43, 37, 44, 47, 46, 49, 45, 48])\n",
    "print(data.shape)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# gaussian transform\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "# scaling\n",
    "scaler = Pipeline([\n",
    "    ('standard', StandardScaler()),\n",
    "    ('minmax', MinMaxScaler())\n",
    "])\n",
    "\n",
    "cols = data.columns.tolist()\n",
    "data = scaler.fit_transform(data)\n",
    "data = pd.DataFrame(data, columns = cols)\n",
    "\n",
    "# Assuming 'df' is your DataFrame with binary features\n",
    "gaussian_cols = [11, 35, 36, 38, 39, 40, 58]\n",
    "transformer = QuantileTransformer(output_distribution='normal', random_state=0)\n",
    "df_gaussianized = transformer.fit_transform(data[gaussian_cols])\n",
    "df_gaussianized = pd.DataFrame(df_gaussianized, columns=gaussian_cols)\n",
    "data = pd.concat([data.drop(columns = gaussian_cols), df_gaussianized], axis = 1)\n",
    "\n",
    "# scaling\n",
    "scaler = Pipeline([\n",
    "    ('standard', StandardScaler()),\n",
    "    ('minmax', MinMaxScaler())\n",
    "])\n",
    "\n",
    "cols = data.columns.tolist()\n",
    "data = scaler.fit_transform(data)\n",
    "data = pd.DataFrame(data, columns = cols)\n",
    "\n",
    "data.columns = [idx for idx, col in enumerate(data.columns)]\n",
    "data['label'] = selected_df['label'].values\n",
    "client_split_indices = np.cumsum([df.shape[0] for df in selected_dfs[:-1]])\n",
    "print(client_split_indices)\n",
    "target_col = 'label'\n",
    "data[target_col], codes = pd.factorize(data[target_col])\n",
    "num_cols = data.columns.tolist()[:-1]\n",
    "cat_cols = []\n",
    "\n",
    "data_config = {\n",
    "    'target': target_col,\n",
    "    'features_idx': list(range(len(data.columns)-1)),\n",
    "    'split_col_idx': [20, 19, 8, 3],\n",
    "    'ms_col_idx': [idx for idx in range(0, data.shape[1]) if data.columns[idx] in num_cols],\n",
    "    'obs_col_idx': [0, 13, 15],\n",
    "    'num_cols': len(num_cols),\n",
    "    'task_type': 'classification',\n",
    "    'clf_type': 'binary',\n",
    "    'data_type': 'tabular',\n",
    "    'client_split_indices': client_split_indices.tolist()\n",
    "}\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ed2b7127",
   "metadata": {},
   "source": [
    "print(data_config)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ffddd9b",
   "metadata": {},
   "source": [
    "data.hist(figsize=(20, 20), bins = 100)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a84e14b2",
   "metadata": {},
   "source": [
    "data.to_csv('./data/vehicle/data_cleaned.csv', index=False)\n",
    "import json\n",
    "with open('./data/vehicle/data_config.json', 'w') as f:\n",
    "    json.dump(data_config, f)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "037d61ea",
   "metadata": {},
   "source": [
    "show_heatmap(data, figsize=(10, 10))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "420812b7",
   "metadata": {},
   "source": [
    "avg_correlation(data)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "369f607a",
   "metadata": {},
   "source": [
    "show_heatmap(data, figsize=(10, 10))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "069f2df5",
   "metadata": {},
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import mean_squared_error, mean_squared_log_error, accuracy_score, roc_auc_score, f1_score\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data.drop(columns = [target_col])\n",
    "y = data[target_col]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train.shape, X_test.shape\n",
    "\n",
    "model = MLPClassifier(hidden_layer_sizes=(128, 128), max_iter=1000, alpha=0.5, random_state=42, verbose=True)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_test, y_pred), roc_auc_score(y_test, y_pred), f1_score(y_test, y_pred))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "9266bf61",
   "metadata": {},
   "source": [
    "## Codon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "da7f1462",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "data = pd.read_csv(\"./data/codon/codon_usage.csv\", sep=',', low_memory=False)\n",
    "data = data.dropna()\n",
    "# data.columns = [str(i) for i in range(data.sh\n",
    "data = data.drop(['SpeciesID', 'Ncodons', 'SpeciesName', 'DNAtype'], axis=1)\n",
    "target_col = 'Kingdom'\n",
    "data = data[data[target_col] != 'plm']\n",
    "data[target_col], codes = pd.factorize(data[target_col])\n",
    "cols = data.corrwith(data[target_col]).abs().sort_values(ascending = False)[0:35].index.tolist()\n",
    "cols.remove(target_col)\n",
    "data = data[cols + [target_col]]\n",
    "\n",
    "num_cols = data.columns.tolist()[:-1]\n",
    "cat_cols = []\n",
    "\n",
    "scaler = Pipeline([\n",
    "    ('standard', StandardScaler()),\n",
    "    ('minmax', MinMaxScaler())\n",
    "])\n",
    "\n",
    "cols = data.columns.tolist()\n",
    "cols.remove(target_col)\n",
    "data[cols] = scaler.fit_transform(data[cols])\n",
    "\n",
    "quant = QuantileTransformer(output_distribution='normal', random_state=0)\n",
    "data[cols] = quant.fit_transform(data[cols])\n",
    "\n",
    "obs_cols = ['CAC', 'UGU', 'UCA']\n",
    "split_cols = ['UGA', 'CUA', 'GAU']\n",
    "\n",
    "data_config = {\n",
    "    'target': target_col,\n",
    "    'features_idx': list(range(len(data.columns)-1)),\n",
    "    'split_col_idx': [data.columns.tolist().index(col) for col in split_cols],\n",
    "    'ms_col_idx': [idx for idx in range(0, data.shape[1]) if data.columns[idx] in num_cols],\n",
    "    'obs_col_idx': [idx for idx in range(0, data.shape[1]) if data.columns[idx] in obs_cols],\n",
    "    'num_cols': len(num_cols),\n",
    "    'task_type': 'classification',\n",
    "    'clf_type': 'multi-class',\n",
    "    'data_type': 'tabular'\n",
    "}\n",
    "\n",
    "print(data_config)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "82cf7719",
   "metadata": {},
   "source": [
    "data.shape"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "db2eae07",
   "metadata": {},
   "source": [
    "data.corrwith(data[target_col]).abs().sort_values(ascending = False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5efa3151",
   "metadata": {},
   "source": [
    "avg_correlation(data)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c1b69f72",
   "metadata": {},
   "source": [
    "show_heatmap(data, figsize=(15, 15))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "65a9c13d",
   "metadata": {},
   "source": [
    "data.hist(figsize=(20, 20), bins = 100)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f0d0842c",
   "metadata": {},
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import mean_squared_error, mean_squared_log_error, accuracy_score, roc_auc_score, f1_score\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data.drop(columns = [target_col])\n",
    "y = data[target_col]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train.shape, X_test.shape\n",
    "\n",
    "model = MLPClassifier(hidden_layer_sizes=(128, 128), max_iter=1000, alpha=0.5, random_state=42, verbose=True)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_test, y_pred))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "39cfe150",
   "metadata": {},
   "source": [
    "data.to_csv('./data/codon/data_cleaned.csv', index=False)\n",
    "import json\n",
    "with open('./data/codon/data_config.json', 'w') as f:\n",
    "    json.dump(data_config, f)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "2d6509a6",
   "metadata": {},
   "source": [
    "## School"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e02dc3a4",
   "metadata": {},
   "source": [
    "import scipy.io\n",
    "from collections import Counter"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d3df635a",
   "metadata": {},
   "source": [
    "mat = scipy.io.loadmat('./data/school/school.mat')\n",
    "# Note that the raw data structure is different from school\n",
    "raw_x, raw_y = mat['X'][0], mat['Y'][0]  # y is exam score\n",
    "assert len(raw_x) == len(raw_y)\n",
    "num_clients = len(raw_x)\n",
    "\n",
    "print('School dataset:')\n",
    "print('number of clients:', num_clients, len(raw_y))\n",
    "print('number of examples:', [len(raw_x[i]) for i in range(num_clients)])\n",
    "print('number of features:', len(raw_x[0][0]))\n",
    "\n",
    "raw_x, raw_y = mat['X'][0], mat['Y'][0]\n",
    "combined_x= np.vstack(raw_x)\n",
    "combiend_y = np.vstack(raw_y)\n",
    "data = pd.DataFrame(combined_x, columns = [f'Feature_{i}' for i in range(combined_x.shape[1])])\n",
    "clients_split_indices = np.cumsum([len(raw_x[i]) for i in range(num_clients-1)])\n",
    "print(clients_split_indices)\n",
    "target_col = 'score'\n",
    "data[target_col] = combiend_y\n",
    "data"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4a7470c7",
   "metadata": {},
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=0.999)\n",
    "\n",
    "cols = data.columns.tolist()[:-1]\n",
    "data_pca = pca.fit_transform(data[cols])\n",
    "\n",
    "data_pca = pd.DataFrame(data_pca, columns = [f'PCA_{i}' for i in range(data_pca.shape[1])])\n",
    "data_pca[target_col] = data[target_col]\n",
    "data_pca"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "85046918",
   "metadata": {},
   "source": [
    "data = data.drop(columns = [\n",
    "    'Feature_27', 'Feature_11', 'Feature_12', 'Feature_13', 'Feature_14', 'Feature_15', 'Feature_16', 'Feature_17', \n",
    "    'Feature_18', 'Feature_19', 'Feature_20', 'Feature_25'\n",
    "], axis = 1)\n",
    "print(data.shape)\n",
    "\n",
    "data.columns = [idx for idx, col in enumerate(data.columns[:-1])] + [target_col] "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6a7a22fd",
   "metadata": {},
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# scaling\n",
    "scaler = Pipeline([\n",
    "    ('standard', StandardScaler()),\n",
    "    ('minmax', MinMaxScaler())\n",
    "])\n",
    "\n",
    "cols = data_pca.columns.tolist()\n",
    "cols.remove(target_col)\n",
    "data_pca[cols] = scaler.fit_transform(data_pca[cols])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "02e717f2",
   "metadata": {},
   "source": [
    "show_heatmap(data_pca, figsize=(15, 15))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8e3b3583",
   "metadata": {},
   "source": [
    "avg_correlation(data_pca)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0d764288",
   "metadata": {},
   "source": [
    "data_pca.corrwith(data_pca[target_col]).abs().sort_values(ascending = False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "483eecf4",
   "metadata": {},
   "source": [
    "data[target_col].hist(bins = 100)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "68452c48",
   "metadata": {},
   "source": [
    "data[target_col].describe()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b8e62974",
   "metadata": {},
   "source": [
    "num_cols = data_pca.columns.tolist()[:-1]\n",
    "data_config = {\n",
    "    'target': target_col,\n",
    "    'features_idx': list(range(len(data_pca.columns)-1)),\n",
    "    'split_col_idx': [9, 7],\n",
    "    'ms_col_idx': [idx for idx in range(0, data_pca.shape[1]) if data_pca.columns[idx] in num_cols],\n",
    "    'obs_col_idx': [6, 10],\n",
    "    'num_cols': len(num_cols),\n",
    "    'task_type': 'regression',\n",
    "    'clf_type': 'none',\n",
    "    'data_type': 'tabular',\n",
    "    'client_split_indices': clients_split_indices.tolist()\n",
    "}\n",
    "print(data_config)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9f9b2707",
   "metadata": {},
   "source": [
    "data_pca.shape"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "08ff04e9",
   "metadata": {},
   "source": [
    "data_pca[target_col] = data[target_col].astype(float)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "379d45e2",
   "metadata": {},
   "source": [
    "data_pca.to_csv('./data/school/data_cleaned_pca.csv', index=False)\n",
    "import json\n",
    "with open('./data/school/data_config_pca.json', 'w') as f:\n",
    "    json.dump(data_config, f)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2bfa8ccd",
   "metadata": {},
   "source": [
    "data_pca.hist(figsize=(20, 20), bins = 100)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "df475eef",
   "metadata": {},
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data.drop(columns = [target_col])\n",
    "y = data[target_col]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train.shape, X_test.shape\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_squared_log_error\n",
    "\n",
    "model = MLPRegressor(hidden_layer_sizes=(128, 128), max_iter=1000, alpha=0.5, random_state=42, verbose=True)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(np.sqrt(mean_squared_error(y_test, y_pred)))"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
